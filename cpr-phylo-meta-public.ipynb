{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The rise of diversity in metabolic platforms across the Candidate Phyla Radiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code supporting Jaffe et al., 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import Tree\n",
    "import matplotlib, re, os, glob, math, wget\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO, SearchIO\n",
    "import seaborn as sns\n",
    "import subprocess as sp\n",
    "import ecopy as ep\n",
    "from skbio.stats.ordination import pcoa\n",
    "sns.set('notebook')\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Part 1:](#Part-1)\n",
    "1.    [process and analyze metadata](#process-and-analyze-metadata)<br>\n",
    "2.    [set up phylogenetic analysis](#set-up-the-phylogenetic-analysis)<br>\n",
    "3.    [manual hmm curation](#manual-hmm-curation)<br>\n",
    "4.    [analyze marker genes](#analyze-marker-gene-set)<br>\n",
    "5.    [create final marker set](#create-final-marker-set)<br>\n",
    "6.    [run initial trees](#run-initial-trees)<br>\n",
    "7.   [run concatenated trees with outgroups](#run-concatenated-trees-with-outgroups)<br>\n",
    "8.   [analyze concatenated trees](#analyze-concatenated-trees)<br>\n",
    "\n",
    "[Part 2:](#Part-2)\n",
    "1.   [create metabolic annotations](#create-metabolic-annotations)<br>\n",
    "2.   [correlating metabolism + phylogeny](#correlating-metabolism-+-phylogeny)<br>\n",
    "3.   [metabolic reference set](#building-a-metabolic-reference-set)<br>\n",
    "4.   [create gene trees](#create-gene-trees)<br>\n",
    "5.   [metabolic case studies](#metabolic-case-studies)<br>\n",
    "6.   [trait depth + homoplasy](#trait-depth-+-homoplasy-metrics)<br>\n",
    "7.   [miscellaneous](#miscellaneous)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N.B. FILL THIS IN WITH PATH\n",
    "# TO GITHUB REPO\n",
    "rootdir = \"\"\n",
    "\n",
    "## FILL THESE IN WITH PATHS TO\n",
    "## DEPENDENCY PROGRAMS:\n",
    "\n",
    "## MAFFT - https://mafft.cbrc.jp/alignment/software/\n",
    "mpath = \"\"\n",
    "## FASTTREEMP - http://www.microbesonline.org/fasttree/\n",
    "ftpath = \"\"\n",
    "## PULLSEQ - https://github.com/bcthomas/pullseq\n",
    "pspath = \"\"\n",
    "## BMGE - https://bmcevolbiol.biomedcentral.com/articles/10.1186/1471-2148-10-210\n",
    "bmgepath = \"\"\n",
    "## PRODIGAL - https://github.com/hyattpd/Prodigal\n",
    "ppath = \"\"\n",
    "## HMMER HMMSEARCH - http://hmmer.org/download.html\n",
    "hpath = \"\"\n",
    "## IQTREE - http://www.iqtree.org/\n",
    "iqpath = \"\"\n",
    "## PROTEIN CLUSTERING - https://github.com/raphael-upmc/proteinClusteringPipeline/tree/master/scripts\n",
    "pcpath = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process and analyze metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the set of 991 dereplicated, quality filtered genomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verify genome set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of the dereplicated genomes\n",
    "drep_genomes = []\n",
    "for genome in glob.glob(rootdir + \"/dereplicated_genomes/*\"):\n",
    "    drep_genomes.append(os.path.basename(genome).replace(\".contigs.fa\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The below dataframe should contain %d metadata records.\" %(len(drep_genomes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read in metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_org_table = pd.read_csv(rootdir + \"final_org_table.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create final taxonomy field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fot = final_org_table\n",
    "# rename cols\n",
    "fot.columns = fot.columns.str.replace('RP Inventory (total: 55)', \"rp_55\", regex=False)\n",
    "fot.columns = fot.columns.str.replace(\"BSCG Inventory (total: 51)\", \"bscg_51\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in cpr tax names to search for\n",
    "with open(rootdir + \"/cpr_names.txt\", \"r\") as infile:\n",
    "    tax_db = [line.strip() for line in infile.readlines()]\n",
    "# remove generic CPR from here\n",
    "tax_db = [item for item in tax_db if item != \"CPR\"]\n",
    "\n",
    "# then define a function to search df tax strings\n",
    "# make it degenerating - from most to least specific\n",
    "\n",
    "def assign_tax(row):\n",
    "    \n",
    "    tax = \"None\"\n",
    "    found = False\n",
    "    # first split up tax string\n",
    "    elements = row[\"taxonomy\"].replace(\" \", \"\").split(\",\")\n",
    "    # then search elements for known taxonomies\n",
    "    for element in elements:\n",
    "        for entry in tax_db:\n",
    "            if entry in element:\n",
    "                tax = entry\n",
    "                found=True\n",
    "                break\n",
    "        if found: break\n",
    "    \n",
    "    # if superphylum level, check bin name for phylum name\n",
    "    found_again = False\n",
    "    if (tax == \"Parcubacteria\") or (tax == \"Microgenomates\") or tax==\"None\":\n",
    "        for entry in tax_db:\n",
    "            if entry in row[\"name\"]:\n",
    "                tax = entry\n",
    "                found_again = True\n",
    "                break\n",
    "                  \n",
    "    return tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fot[\"final_tax\"] = fot.apply(assign_tax, axis=1)\n",
    "# at this point good to check assignments including Microgenomates, Parcubacteria, None\n",
    "# make some manual edits\n",
    "fot[\"final_tax\"] = fot[\"final_tax\"].replace({\"WS6\":\"Dojkabacteria\", \"TM7\":\"Saccharibacteria\", \\\n",
    "                    \"SR1\":\"Absconditabacteria\", \"SM2F11\":\"Doudnabacteria\", \"Djokabacteria\":\"Dojkabacteria\", \\\n",
    "                     \"CPR02\":\"CPR1\", \"CPR03\":\"CPR3\", \"BD1-5\": \"Gracilibacteria\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up the phylogenetic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the genome set and metadata created, we can set up the preliminary phylogenetic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmdir(path):\n",
    "    \n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create protein database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"/proteins/\")\n",
    "cmdir(rootdir + \"/proteins/prodigal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(rootdir + \"prodigal_wrapper.sh\", \"w\") as wrapper:\n",
    "    for genome in glob.glob(rootdir + \"/dereplicated_genomes/*\"):\n",
    "        # if alternatively coded, repredict proteins with code 25\n",
    "        if \"Gracilibacteria\" in genome or \"SR1\" in genome or \"BD1-5\" in genome:\n",
    "            code = \"25\"\n",
    "        else:\n",
    "            code = \"11\"\n",
    "        call = ppath + \" -i \" + genome + \" -m -g \" + code + \" -a \" + \\\n",
    "            rootdir + \"/proteins/prodigal/\" + os.path.basename(genome).replace(\".fa\",\".faa\")\n",
    "        wrapper.write(call + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that everything made it\n",
    "count = 0\n",
    "for protein_file in glob.glob(rootdir + \"/proteins/prodigal/*\"):\n",
    "    count +=1\n",
    "print(\"Protein files were transferred successfully for %d genomes.\" %(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify UBA genomes contig names - non unique\n",
    "for proteome in glob.glob(rootdir + \"/proteins/prodigal/UBA*\"):\n",
    "    \n",
    "    genome_name = os.path.basename(proteome).split(\".\")[0]\n",
    "    # construct call to sed to modify- nb escape backslash\n",
    "    call = \"cat \" + proteome + \" | sed -r 's/>(.+)/>\" + genome_name + \"_\\\\1/' > \" + proteome + \".mod\"\n",
    "    sp.call(call, shell=True)\n",
    "    # remove originals\n",
    "    call2 = \"rm \" + proteome\n",
    "    sp.call(call2, shell=True)\n",
    "    # rename modified to take their place\n",
    "    call3 = \"mv \" + proteome + \".mod \" + proteome\n",
    "    sp.call(call3, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create concatenated protein file for hmm\n",
    "call = \"cat \" + rootdir + \"/proteins/prodigal/* > \" + rootdir + \"/proteins/all_proteins.faa\"\n",
    "sp.call(call, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run hmms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tigr_dict = {\"rpol\":{\"TIGR02013\":\"rpoB\", \"TIGR02386\": \"rpoC\"}, \n",
    "             \"rp16\":{\"PF00410\": \"rpS8\",\"PF00281\":\"rpL5\",\"TIGR00060\":\"rpL18\",\"TIGR01009\":\"rpS3\",\n",
    "            \"TIGR01044\":\"rpL22\",\"TIGR01049\":\"rpS10\",\"TIGR01050\":\"rpS19\",\"TIGR01067\":\"rpL14\",\n",
    "            \"TIGR01071\":\"rpL15\",\"TIGR01079\":\"rpL24\",\"TIGR01164\":\"rpL16\",\"TIGR01171\":\"rpL2\",\n",
    "            \"TIGR03625\":\"rpL3\",\"TIGR03635\":\"rpS17\",\"TIGR03654\":\"rpL6\",\"TIGR03953\":\"rpL4\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hmm(hmm_name):\n",
    "    \n",
    "    call = hpath + \" --tblout \" + rootdir + \"/proteins/hmm_results/\" + hmm_name + \\\n",
    "        \".results \" + rootdir + \"hmms/\" + hmm_name + \\\n",
    "        \".HMM \" + rootdir + \"/proteins/all_proteins.faa\"\n",
    "    sp.call(call, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"/proteins/hmm_results/\")\n",
    "\n",
    "for key, items in tigr_dict.items():\n",
    "    for hmm in items:\n",
    "        run_hmm(hmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_hmm(result_table):\n",
    "    \n",
    "    temp = {}\n",
    "    count = 0\n",
    "    \n",
    "    # parse each result file using searchio\n",
    "    for result in SearchIO.parse(result_table, \"hmmer3-tab\"):\n",
    "        for item in result.hits:\n",
    "            temp[count] = {\"gene\": item.id, \"score\": item.bitscore, \"eval\": item.evalue}\n",
    "            count += 1\n",
    "            \n",
    "    return(pd.DataFrame.from_dict(temp, orient=\"index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_results = {}\n",
    "\n",
    "for hmm_result in glob.glob(rootdir + \"/proteins/hmm_results/*results\"):\n",
    "    hmm_results[os.path.basename(hmm_result).split(\".\")[0]] = parse_hmm(hmm_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize and process results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(rootdir + \"/proteins/hmm_results/wrapper.sh\", \"w\") as wrapper:\n",
    "    \n",
    "    for key in hmm_results.keys():\n",
    "    \n",
    "        table = hmm_results[key]\n",
    "        table[\"position\"] = table.index\n",
    "        table[\"significant\"] = table[\"eval\"].apply(lambda x: x < 0.05)\n",
    "        #table[\"logeval\"] = table[\"eval\"].apply(lambda x: math.log10(x + 1e-200))\n",
    "        outdir_basename = rootdir + \"/proteins/hmm_results/\" + key\n",
    "\n",
    "        ## PARSE OUT SIGNIFICANT HITS\n",
    "        with open(outdir_basename + \".sighits.txt\",\"w\") as outfile:\n",
    "            for idx,rowe in table[table[\"significant\"]==True].iterrows():\n",
    "                outfile.write(rowe[\"gene\"] + \"\\n\")\n",
    "\n",
    "        call = pspath + \" -n \" + outdir_basename + \".sighits.txt -i \" + \\\n",
    "            rootdir + \"/proteins/all_proteins.faa > \" + \\\n",
    "            outdir_basename + \".sighits.faa\"\n",
    "        sp.call(call,shell=True)\n",
    "\n",
    "        ## NOW ALIGN + TREE BUILD\n",
    "        call = mpath + \" --thread 6 --reorder \" + outdir_basename + \".sighits.faa > \" + \\\n",
    "            outdir_basename + \".sighits.mafft\"\n",
    "        wrapper.write(call + \"\\n\")\n",
    "\n",
    "        # BUILD TREE\n",
    "        call = ftpath + \" \" + outdir_basename + \".sighits.mafft > \" + \\\n",
    "            outdir_basename + \".sighits.tre\"   \n",
    "        wrapper.write(call + \"\\n\")\n",
    "\n",
    "        # GENERATE ITOL DATA\n",
    "        with open(outdir_basename + \".itol.txt\", \"w+\") as itol:\n",
    "            itol.write(\"DATASET_SIMPLEBAR\\nSEPARATOR COMMA\\nDATASET_LABEL,hmm_score\\nSHOW_VALUE,1\\nCOLOR,#ff0000\\nDATA\\n\")\n",
    "            for idx,rowe in table[table[\"significant\"]==True].iterrows():\n",
    "                itol.write(rowe[\"gene\"] + \",\" + str(rowe[\"score\"]) + \"\\n\")\n",
    "\n",
    "        #THEN PLOT\n",
    "        plt.figure(figsize=(10,3))       \n",
    "        sns.regplot(\"position\", \"score\", data=table, fit_reg=False, scatter_kws={'s':60}, color=\"blue\")\n",
    "        plt.xlabel(\"rank\")\n",
    "        plt.xticks()\n",
    "        ax2 = plt.twinx()\n",
    "        ax2.set_ylim(0, 0.05)\n",
    "        ax2.grid(False)\n",
    "        sns.regplot(\"position\", \"eval\", data=table, fit_reg=False, scatter_kws={'s':60}, ax=ax2, color=\"orange\")\n",
    "        plt.title(\"%s\" %(key))\n",
    "        plt.savefig(outdir_basename + \".png\", format=\"png\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chmod +x wrapper.sh and add `export OMP_NUM_THREADS=6`. Then, based on these results, we can then set cutoffs for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# manual hmm curation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With HMMs run, visualized, we now perform a manual hmm curation using the above figures and hmm scores mapped to tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get hmm info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cutoffs(row, cutoff_type):\n",
    "    \n",
    "    cutoff = \"none\"\n",
    "    basedir = rootdir + \"/hmms/\"\n",
    "    # extract noise cutoff\n",
    "    for line in open(basedir + row[\"hmm\"] + \".HMM\"):\n",
    "        m = re.search(\"^\" + cutoff_type + \"\\s+(\\S+).+\", line)\n",
    "        if m:\n",
    "            cutoff = float(m.group(1))\n",
    "    return cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_dict = pd.DataFrame.from_dict(tigr_dict, orient=\"columns\")\n",
    "hmm_dict = hmm_dict.reset_index().fillna(\"None\")\n",
    "hmm_dict.columns = [\"hmm\", \"rpol\", \"rp16\"]\n",
    "hmm_dict[\"name\"] = hmm_dict.apply(lambda x: x[\"rpol\"] if x[\"rpol\"] != \"None\" else x[\"rp16\"], axis=1)\n",
    "hmm_dict[\"set\"] = hmm_dict.apply(lambda x: \"rpol\" if x[\"rpol\"] != \"None\" else \"rp16\", axis=1)\n",
    "hmm_dict = hmm_dict.drop([\"rpol\", \"rp16\"], axis=1)\n",
    "hmm_dict[\"noise_cutoff\"] = hmm_dict.apply(lambda x: get_cutoffs(x, \"NC\"), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter hmm results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_cutoffs = {\"PF00281\": \"all_significant\", \"PF00410\": 59.6, \"TIGR00060\": 39, \"TIGR01009\": 29, \"TIGR01044\": \"all_significant\", \\\n",
    "                 \"TIGR01049\": \"all_significant\", \"TIGR01050\": \"all_significant\", \"TIGR01067\": \"all_significant\", \"TIGR01071\": 29.4,\n",
    "                 \"TIGR01079\": 23.3, \"TIGR01164\": \"all_significant\", \"TIGR01171\": \"all_significant\", \"TIGR02013\": 25.5, \n",
    "                 \"TIGR02386\": 18.1, \"TIGR03625\": \"all_significant\",\"TIGR03635\": \"all_significant\", \"TIGR03654\": \"all_significant\", \n",
    "                 \"TIGR03953\": \"all_significant\"}\n",
    "\n",
    "hmm_results_filt = {}\n",
    "eff_cutoffs = {}\n",
    "\n",
    "for key in hmm_results.keys():\n",
    "    table = hmm_results[key]\n",
    "    table[\"significant\"] = table[\"eval\"].apply(lambda x: x < 0.05)\n",
    "    if manual_cutoffs[key] == \"all_significant\":\n",
    "        hmm_results_filt[key] = table[table[\"significant\"]==True]\n",
    "        effective_cutoff = min(list(table[table[\"significant\"]==True][\"score\"]))\n",
    "    else:\n",
    "        effective_cutoff = manual_cutoffs[key]\n",
    "        # not inclusive - cutoff = highest outlier entry\n",
    "        hmm_results_filt[key] = table[table[\"score\"] > effective_cutoff]\n",
    "    # save out effective cutoffs\n",
    "    eff_cutoffs[key] = effective_cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"/proteins/hmm_curation/\")\n",
    "\n",
    "for key in hmm_results_filt.keys():\n",
    "    \n",
    "    # first create sequence name list\n",
    "    filepath = rootdir + \"/proteins/hmm_curation/\" + key + \".filt.names\"\n",
    "    with open(filepath, \"w\") as outfile:\n",
    "        for index, row in hmm_results_filt[key].iterrows():\n",
    "            outfile.write(row[\"gene\"] + \"\\n\")\n",
    "    \n",
    "    # then send to pullseq\n",
    "    call = pspath + \" -n \" + filepath + \" -i \" + rootdir + \"/proteins/all_proteins.faa > \" + \\\n",
    "        rootdir + \"proteins/hmm_curation/\" + key + \".filt.faa\"\n",
    "    sp.call(call, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### revisualize - Supp. Fig 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add effective cutoffs to df\n",
    "hmm_dict[\"manual_cutoff\"] = hmm_dict[\"hmm\"].apply(lambda x: eff_cutoffs[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hmms to plot\n",
    "exemplars = [\"TIGR01009\", \"TIGR02013\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"/figures/\")\n",
    "\n",
    "for key,row in hmm_dict.iterrows():\n",
    "        \n",
    "        if row[\"hmm\"] in exemplars:\n",
    "            \n",
    "            table = hmm_results[row[\"hmm\"]]\n",
    "            table[\"position\"] = table.index\n",
    "            table[\"significant\"] = table[\"eval\"].apply(lambda x: x < 0.05)\n",
    "\n",
    "            #THEN PLOT\n",
    "            sns.set(font_scale=1.5)\n",
    "            sns.set_style(\"ticks\")\n",
    "            plt.figure(figsize=(20,6)) \n",
    "            \n",
    "            for cutoff in [\"noise_cutoff\", \"manual_cutoff\"]:\n",
    "                plt.axhline(float(row[cutoff]), ls='--', color=\"grey\")\n",
    "                # position text with slight adjustments\n",
    "                plt.text(int(max(table[\"position\"]))*0.01,float(row[cutoff]) + int(max(table[\"score\"]))*0.02, cutoff.replace(\"_\", \" \") + \\\n",
    "                             \" at \" + str(row[cutoff]), color=\"grey\")\n",
    "                \n",
    "            sns.regplot(\"position\", \"score\", data=table, fit_reg=False, scatter_kws={'s':40}, color=\"blue\")\n",
    "            plt.xlabel(\"rank\")\n",
    "            plt.xticks()\n",
    "            ax2 = plt.twinx()\n",
    "            ax2.set_ylim(0, 0.05)\n",
    "            ax2.grid(False)\n",
    "            sns.regplot(\"position\", \"eval\", data=table, fit_reg=False, scatter_kws={'s':40}, ax=ax2, color=\"orange\")\n",
    "            plt.title(\"%s\" %(row[\"hmm\"]))\n",
    "            outdir = rootdir + \"/figures/\" + row[\"hmm\"] + \"_results_annotated\"\n",
    "            plt.savefig(outdir + \".svg\", format=\"svg\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyze marker gene set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a manually curated marker gene set that can be further analyzed and refined, if necessary. First, let's attach metadata, which requires a scaf2bin file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a scaf2bin for all genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaf2bin = {}\n",
    "\n",
    "# iterate through genomes\n",
    "for genome in glob.glob(rootdir + \"/dereplicated_genomes/*\"):\n",
    "    \n",
    "    genome_name = os.path.basename(genome).split(\".\")[0]\n",
    "    #iterate through scaffolds\n",
    "    for record in SeqIO.parse(open(genome, \"r\"), \"fasta\"):\n",
    "        # modify scaffold names for UBA\n",
    "        if \"UBA\" in genome_name:\n",
    "            scaf_name = genome_name + \"_\" + record.description\n",
    "        else:\n",
    "            # get rid of junk\n",
    "            m = re.search(\"(\\S+).+\", record.description)\n",
    "            scaf_name = m.group(1)\n",
    "        # then add to dict\n",
    "        scaf2bin[scaf_name] = genome_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get gene info and attach metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaffold(gene):\n",
    "    if gene != \"None\":\n",
    "        try: return re.search(\"(.+?)_[0-9]+$\", gene).group(1)\n",
    "        except: print(gene)\n",
    "\n",
    "def retrieve_bin(scaffold, scaf_dict):\n",
    "    try: return scaf_dict[scaffold]\n",
    "    except: print(\"%s not found in scaf2bin!\" %(scaffold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_lengths = {}\n",
    "count=0\n",
    "# read in gene lengths\n",
    "for seq_file in glob.glob(rootdir + \"/proteins/hmm_curation/*.filt.faa\"):\n",
    "    name = os.path.basename(seq_file).split(\".\")[0]\n",
    "    for record in SeqIO.parse(open(seq_file,\"r\"), \"fasta\"):\n",
    "        m = re.search(\"(\\S+).+\", record.description)\n",
    "        gene_lengths[count] = {\"gene\": m.group(1), \"len\":len(record.seq), \"hmm\":name, \"seq\": str(record.seq)}\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df\n",
    "len_df = pd.DataFrame.from_dict(gene_lengths, orient=\"index\")\n",
    "# attach scaffold name\n",
    "len_df[\"scaffold\"] = len_df[\"gene\"].apply(scaffold)\n",
    "# attach bin name\n",
    "len_df[\"bin\"] = len_df[\"scaffold\"].apply(lambda x: retrieve_bin(x,scaf2bin))\n",
    "merged = pd.merge(len_df, fot, left_on=\"bin\", right_on=\"name\")\n",
    "# and subset\n",
    "merged = merged[[\"gene\",\"scaffold\",\"hmm\", \"len\", \"bin\", \"final_tax\", \"rp_55\", \"bscg_51\",\"seq\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check gene lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, items in tigr_dict.items():\n",
    "    merged_subset = merged[merged.hmm.isin(items)]\n",
    "    # and plot\n",
    "    plt.figure(figsize=(16,6))\n",
    "    sns.violinplot(x=\"hmm\", y=\"len\", data=merged_subset, palette=\"Set2\")\n",
    "    sns.stripplot(x=\"hmm\", y=\"len\", data=merged_subset, palette=\"Set2\", jitter=True, size=2, linewidth=1, color=\"black\")\n",
    "    plt.ylabel(\"protein length (aa)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analyze partial genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some genomes have marker genes that are split, possibly due to errors in assembly. How common is this problem? Some split genes will have multiple hmm hits on the same contig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiple(row):\n",
    "    mult = False\n",
    "    for i in range(1,len(row)):\n",
    "        if row[i] > 1:\n",
    "            mult=True\n",
    "    return mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate hmm hit counts by scaffold\n",
    "partial = len_df[[\"gene\", \"len\", \"hmm\"]]\n",
    "partial[\"ones\"] = 1\n",
    "pp = partial.pivot(\"gene\", \"hmm\", \"ones\")\n",
    "pp = pp.fillna(0)\n",
    "pp[\"gene\"] = pp.index\n",
    "pp[\"scaffold\"] = pp[\"gene\"].apply(scaffold)\n",
    "pp= pp.drop(\"gene\", axis=1)\n",
    "ppg = pp.groupby(\"scaffold\", as_index=False).sum()\n",
    "ppg[\"mult\"] = ppg.apply(get_multiple, axis=1)\n",
    "# select scaffolds with multiple hits of a single hmm\n",
    "mult = ppg[ppg[\"mult\"] == True]\n",
    "mult = mult.drop(\"mult\", axis=1)\n",
    "print(\"There are %d scaffolds with multiple copies of at least one of the marker genes in both sets.\" %(len(mult)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create final marker set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### editing split genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strand(gene):\n",
    "    try: return strand_dict[gene]\n",
    "    except: print(\"%s not found in strand_dict!\" %(gene))\n",
    "        \n",
    "def check_consecutive(gene_list):\n",
    "    # get gene no's and sort\n",
    "    ns = sorted([int(item.split(\"_\")[-1]) for item in gene_list])\n",
    "    # calculate consec\n",
    "    diff = ns[-1] - ns[0]\n",
    "    if diff == (len(ns)-1):\n",
    "        return True\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull strandedness information\n",
    "strand_dict = {}\n",
    "\n",
    "for record in SeqIO.parse(open(rootdir + \"/proteins/all_proteins.faa\",\"r\"), \"fasta\"):\n",
    "    gene_name = record.description.split(\" # \")[0]\n",
    "    strand = record.description.split(\" # \")[3]\n",
    "    strand_dict[gene_name] = strand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_genes = []\n",
    "removed_genes = []\n",
    "\n",
    "cmdir(rootdir + \"/proteins/partials/\")\n",
    "\n",
    "for key, items in tigr_dict.items():\n",
    "    \n",
    "    for hmm in items: \n",
    "        \n",
    "        # define metadata\n",
    "        name = hmm\n",
    "        out_name = hmm + \".modified.faa\"\n",
    "        \n",
    "        # only do this for hmms with split scaf results\n",
    "        if len(mult[mult[name]>1]) > 0:\n",
    "            \n",
    "            with open(rootdir + \"/proteins/partials/\" + out_name, \"w\") as outfile: \n",
    "\n",
    "                # iterate through scaffolds with multiple hits for hmm of interest\n",
    "                for index, row in mult[mult[name]>1].iterrows():\n",
    "                    # get sub table from merged with component genes\n",
    "                    sub_table = merged[(merged.scaffold == row[\"scaffold\"]) & (merged[\"hmm\"] == name)]\n",
    "                    # then iterate through those genes\n",
    "                    strands = []\n",
    "                    genes = []\n",
    "                    for i, r in sub_table.iterrows():\n",
    "                        genes.append(r[\"gene\"])\n",
    "                        # then get the strandedness\n",
    "                        strand = get_strand(r[\"gene\"])\n",
    "                        strands.append(strand)\n",
    "                    # only proceed if on one strand and if consecutive\n",
    "                    if ((len(set(strands)) == 1) & (check_consecutive(genes)==True)):\n",
    "                        # sort by gene number\n",
    "                        genes.sort(key= lambda x: int(x.split('_')[-1]))\n",
    "                        # if comp strand, reverse\n",
    "                        if list(set(strands))[0] == \"-1\":\n",
    "                            genes.reverse()\n",
    "                        \n",
    "                        count = 0\n",
    "                        for gene in genes:\n",
    "                            seq = sub_table[sub_table.gene==gene][\"seq\"].values[0]\n",
    "                            if \"XX\" in seq:\n",
    "                                if count == 0: #most upstream gene\n",
    "                                    outfile.write(str(\">\" + gene + \"\\n\"))\n",
    "                                    # trim off post error\n",
    "                                    outfile.write(sub_table[sub_table.gene==gene][\"seq\"].values[0].split(\"X\")[0] + \"\\n\")\n",
    "                                    modified_genes.append(gene)\n",
    "                                    new_seq = sub_table[sub_table.gene==gene][\"seq\"].values[0].split(\"X\")[0]\n",
    "                                else: # if downstream, drop\n",
    "                                    removed_genes.append(gene)\n",
    "                            count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((removed_genes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### revisit hmm results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keep track of which\n",
    "# hmms got merged seqs\n",
    "hmms_w_merged = []\n",
    "\n",
    "for key in hmm_results_filt.keys():\n",
    "    \n",
    "    # add merged info to df\n",
    "    table = hmm_results_filt[key]\n",
    "    table[\"position\"] = table.index\n",
    "    table[\"merged\"] = table[\"gene\"].apply(lambda x: x in modified_genes)\n",
    "    # now plot\n",
    "    if len(table[table[\"merged\"] == True]) > 0:\n",
    "        hmms_w_merged.append(key)\n",
    "        \n",
    "        for variable in [\"score\"]:\n",
    "            \n",
    "            sns.set(font_scale=1.5)\n",
    "            sns.set_style(\"ticks\")\n",
    "            sns.lmplot(\"position\", variable, data=table, fit_reg=False, hue=\"merged\",scatter_kws={'s':20}, palette=\"coolwarm\", size = 3, aspect=3, legend=False)\n",
    "            plt.title(\"%s %s\" %(key, variable))\n",
    "            #if variable == \"eval\":\n",
    "                #plt.ylim([0,10e-20])\n",
    "            plt.xlabel(\"rank\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### edit the sequence set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, item in tigr_dict.items():\n",
    "    \n",
    "    for hmm in item:\n",
    "        \n",
    "        # initialize final seq file\n",
    "        filename = hmm + \".final.faa\"\n",
    "        with open(rootdir + \"/proteins/partials/\" + filename, \"w\") as outfile:\n",
    "            \n",
    "            if hmm in hmms_w_merged:\n",
    "                # first add new sequences for those w/ merged\n",
    "                for merged_seq in SeqIO.parse(open(rootdir + \"/proteins/partials/\" + hmm + \".modified.faa\", \"r\"),\"fasta\"):\n",
    "                    outfile.write(\">\" + merged_seq.description + \"\\n\" + str(merged_seq.seq) + \"\\n\")\n",
    "\n",
    "            # then add old sequences, but not ones that were merged\n",
    "            for old_seq in SeqIO.parse(open(rootdir + \"/proteins/hmm_curation/\" + hmm + \".filt.faa\", \"r\"),\"fasta\"):\n",
    "                # pull clean headers\n",
    "                m = re.search(\"(\\S+).+\", old_seq.description)\n",
    "                if m.group(1) not in modified_genes and m.group(1) not in removed_genes:\n",
    "                    outfile.write(\">\" + old_seq.description + \"\\n\" + str(old_seq.seq) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reattach metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow for and in merged headers\n",
    "def rescaffold(gene):\n",
    "    if gene != \"None\":\n",
    "        try: return re.search(\"(.+?)_[0-9and]+$\", gene).group(1)\n",
    "        except: print(gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_genes = {}\n",
    "count=0\n",
    "# read in gene lengths\n",
    "for seq_file in glob.glob(rootdir + \"/proteins/partials/*.final.faa\"):\n",
    "    name = os.path.basename(seq_file).split(\".\")[0]\n",
    "    for record in SeqIO.parse(open(seq_file,\"r\"), \"fasta\"):\n",
    "        m = re.search(\"(\\S+).*\", record.description)\n",
    "        trimmed_genes[count] = {\"gene\": m.group(1), \"len\": len(record.seq), \"hmm\":name, \"seq\": str(record.seq)}\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df\n",
    "tdf = pd.DataFrame.from_dict(trimmed_genes, orient=\"index\")\n",
    "# attach scaffold name\n",
    "tdf[\"scaffold\"] = tdf[\"gene\"].apply(rescaffold)\n",
    "# attach bin name\n",
    "tdf[\"bin\"] = tdf[\"scaffold\"].apply(lambda x: retrieve_bin(x, scaf2bin))\n",
    "tm = pd.merge(tdf, fot, left_on=\"bin\", right_on=\"name\")\n",
    "tms = tm[[\"bin\", \"gene\",\"scaffold\",\"hmm\",\"len\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deal with multiples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general strategy here is to summarize the contents of each scaffold in the genome, undergo a selection process by which genes on the same scaffold are preferenced, then subsequently ones with more data (longer combined length)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> N.B. </b> Takes a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = []\n",
    "count = 0\n",
    "\n",
    "# iterate through genomes\n",
    "for genome in list(set(tms[\"bin\"])):\n",
    "    \n",
    "    count+=1\n",
    "    print('Processing genome [%d]\\r'%count, end=\"\")\n",
    "    \n",
    "    # get dataframe subset and scaffold\n",
    "    df = tms[tms.bin==genome]\n",
    "    df[\"scaf\"] = df[\"gene\"].apply(rescaffold)\n",
    "    scafs = {}\n",
    "    # iterate through scaffolds represented and add gene name + length\n",
    "    for key, row in df.iterrows():\n",
    "        if row[\"scaf\"] not in scafs:\n",
    "            scafs[row[\"scaf\"]] = {\"name\": row[\"scaf\"], \"genes\": {row[\"hmm\"]}, \"total_seq\": int(row[\"len\"])}\n",
    "        else:\n",
    "            scafs[row[\"scaf\"]][\"genes\"].add(row[\"hmm\"])\n",
    "            scafs[row[\"scaf\"]][\"total_seq\"] += int(row[\"len\"])\n",
    "    \n",
    "    # for a given genome, which genes are present across all scafs?\n",
    "    all_genes = []\n",
    "    for key in scafs.keys():\n",
    "        all_genes += list(scafs[\n",
    "            key][\"genes\"])\n",
    "    \n",
    "    # next, choose the marker genes\n",
    "    scafs_list = [scafs[item] for item in scafs.keys()]\n",
    "    # multiple sort, first on # genes on contig, then for total length \n",
    "    newlist = sorted(scafs_list, key=lambda x: (len(x[\"genes\"]), x[\"total_seq\"]), reverse=True) \n",
    "    found = []\n",
    "    scafs2keep = []\n",
    "    for scaf in newlist:\n",
    "        # stop when you get all genes needed\n",
    "        if len(set(found)) == len(set(all_genes)):\n",
    "            break\n",
    "        for gene in list(scaf[\"genes\"]):\n",
    "            if gene not in found:\n",
    "                found.append(gene)\n",
    "                scafs2keep.append(scaf[\"name\"])\n",
    "    \n",
    "    # second pass - removing duplicates on same scaf\n",
    "    dfs = df[df.scaffold.isin(scafs2keep)]\n",
    "    final = {}\n",
    "    for key, row in dfs.iterrows():\n",
    "        if row[\"hmm\"] not in final:\n",
    "            final[row[\"hmm\"]] = [{\"name\": row[\"gene\"], \"len\": row[\"len\"]}]\n",
    "        else: final[row[\"hmm\"]].append({\"name\": row[\"gene\"], \"len\": row[\"len\"]})\n",
    "    for hmm in final.keys():\n",
    "        # take one that's longest - most info content\n",
    "        sorted_genes = sorted(final[hmm], key=lambda x: x[\"len\"], reverse=True) \n",
    "        to_keep.append(sorted_genes[0][\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now filter on the recovered genes\n",
    "tms2 = tms[tms[\"gene\"].isin(to_keep)]\n",
    "tmsc2 = tms2.groupby(\"bin\", as_index=False).aggregate({\"gene\":\"count\"})\n",
    "tmsc2.sort_values(\"gene\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all bins that previously had multiple marker gene copies have just one for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check final data matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've dealt with duplicate entries, we can pivot the dataset on bin name and re-attach to our metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genes to keep are processed, but in the to_keep list\n",
    "tmsf = tms[tms[\"gene\"].isin(to_keep)]\n",
    "# now iteratively add this info back to final org table\n",
    "base = fot\n",
    "for hmm in set(tmsf[\"hmm\"]):\n",
    "    df = tmsf[tmsf[\"hmm\"] == hmm][[\"bin\", \"gene\"]]\n",
    "    df.columns = [\"name\", hmm]\n",
    "    # important left join - keep all bins\n",
    "    base = pd.merge(base,df, on=\"name\", how=\"left\")\n",
    "base = base.fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_markers(row,dataset):\n",
    "    count = 0\n",
    "    for hmm in dataset:\n",
    "        if row[hmm] != \"None\":\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "base = base.drop_duplicates()\n",
    "\n",
    "# tabulate marker set completeness for each dataset\n",
    "for dataset, items in tigr_dict.items():\n",
    "    col_name = dataset + \"_count\"\n",
    "    base[col_name] = base.apply(lambda x: count_markers(x, items.keys()), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run initial trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to run some preliminary trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### edit sequence set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get names of all genes\n",
    "final_markers = []\n",
    "for key, row in base.iterrows():\n",
    "    for dataset, item in tigr_dict.items():\n",
    "        for hmm in item:\n",
    "            if row[hmm] != \"None\":\n",
    "                final_markers.append(row[hmm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"/proteins/trimmed\")\n",
    "\n",
    "for dataset, item in tigr_dict.items():\n",
    "    for hmm in item:\n",
    "        # initialize final final seq file\n",
    "        filename = hmm + \".trimmed.faa\"\n",
    "        with open(rootdir + \"/proteins/trimmed/\" + filename, \"w\") as outfile:\n",
    "            # then add old sequences, but not ones that were filtered out\n",
    "            for old_seq in SeqIO.parse(open(rootdir + \"/proteins/partials/\" + hmm + \".final.faa\", \"r\"),\"fasta\"):\n",
    "                # pull clean headers\n",
    "                m = re.search(\"(\\S+).*\", old_seq.description)\n",
    "                if m.group(1) in final_markers:\n",
    "                    outfile.write(\">\" + old_seq.description + \"\\n\" + str(old_seq.seq) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### align and trim with bmge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(rootdir + \"/proteins/trimmed/align.sh\", \"w\") as outfile:\n",
    "    for file in glob.glob(rootdir + \"/proteins/trimmed/*trimmed*\"):\n",
    "        name = file.replace(\"faa\",\"mafft\")\n",
    "        call = mpath + \" --thread 10 --retree 2 --reorder \" + file + \" > \" + name\n",
    "        #sp.call(call, shell=True)\n",
    "        outfile.write(call + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir+\"/trees/prelim_trees\")\n",
    "\n",
    "for alignment in glob.glob(rootdir + \"/proteins/trimmed/*mafft*\"):\n",
    "    name = alignment.replace(\"mafft\",\"bmge.mafft\")\n",
    "    call = \"java -jar \" + bmgepath + \" -i \" + alignment + \" -t AA -m BLOSUM30 -of \" + name\n",
    "    sp.call(call, shell=True)\n",
    "    # copy over to tree building dir\n",
    "    call2 = \"cp \" + name + \" \" + name.replace(\"proteins/trimmed\", \"trees/prelim_trees\")\n",
    "    sp.call(call2, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### send single gene trees to cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. Uses qsub, modify below command if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trimmed_alignment in glob.glob(rootdir + \"/trees/prelim_trees/*bmge*\"):\n",
    "\n",
    "    basename = os.path.basename(trimmed_alignment).split(\".\")[0]\n",
    "    outpath = rootdir + \"/trees/prelim_trees/\" + basename\n",
    "    call = \"echo '\" + iqpath + \" -s \" + trimmed_alignment + \" -m TEST -nt AUTO -st AA -bb 1500 -pre \" + outpath + \"' | qsub -V -N \" + basename\n",
    "    sp.call(call, shell=True)\n",
    "    #print(call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modify treefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build name dict\n",
    "name_dict = {}\n",
    "for key, row in base.iterrows():\n",
    "    for key,item in tigr_dict.items():\n",
    "        for hmm in item:\n",
    "            # small changes to match post bmge names\n",
    "            #mod = row[hmm].replace(\".\", \"_\").replace(\"-\", \"_\")\n",
    "            mod = row[hmm]\n",
    "            name_dict[mod] = row[\"final_tax\"] + \":\" + row[\"name\"]\n",
    "\n",
    "def rename_leaf(leaf):\n",
    "    try: return name_dict[leaf]\n",
    "    except: print(\"%s not found!\" %(leaf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prelim_tree in glob.glob(rootdir + \"/trees/prelim_trees/*treefile*\"):\n",
    "    \n",
    "    #read tree in\n",
    "    t = Tree(prelim_tree)\n",
    "    # simplify leaf names\n",
    "    for leaf in t:\n",
    "        #trim leaf name\n",
    "        trim = leaf.name.split(\"___\")[0]\n",
    "        leaf.name = rename_leaf(trim)\n",
    "\n",
    "    # now write out the tree\n",
    "    t.write(outfile=prelim_tree.replace(\"treefile\",\"renamed.treefile\"),format=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run concatenated trees with outgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to add in other bacteria as outgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cpr(header):\n",
    "    \n",
    "    found = False\n",
    "    for taxon in list(set(base[\"final_tax\"])):\n",
    "        if taxon in header or \"Saccharimonas\" in header:\n",
    "            found = True\n",
    "    return found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge phylogenetic datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set source for genome info\n",
    "in_wd = rootdir + \"/bac175/\"\n",
    "# set source for outfiles\n",
    "out_wd = rootdir + \"/trees/bac175_outgroup/\"\n",
    "cmdir(out_wd)\n",
    "prefix = \"BAC175\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(out_wd + \"/align.sh\", \"w+\") as outfile:\n",
    "    \n",
    "    for hmm in glob.glob(in_wd + \"hmm_results/*faa\"):\n",
    "    \n",
    "        # concatenate trimmed data with references\n",
    "        basename = os.path.basename(hmm).split(\".\")[0]\n",
    "        call = \"cat \" + hmm + \" \" + rootdir + \"/proteins/trimmed/\" + basename + \\\n",
    "            \".trimmed.faa > \" + out_wd + basename + \".\" + prefix + \".concat.faa\"\n",
    "        sp.call(call, shell=True)\n",
    "\n",
    "        # generate alignment call\n",
    "        aln_name = out_wd + basename + \".\" + prefix + \".concat.mafft\"\n",
    "        call = mpath + \" --thread 8 --retree 2 --reorder \" + out_wd + basename + \".\" + prefix + \".concat.faa\" + \" > \" + aln_name\n",
    "        outfile.write(call + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alignment in glob.glob(out_wd + \"*mafft*\"):\n",
    "    name = alignment.replace(\"mafft\", \"bmge.mafft\")\n",
    "    call = \"java -jar \" + bmgepath + \" -i \" + alignment + \" -t AA -m BLOSUM30 -of \" + name\n",
    "    sp.call(call, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bac_scaf2bin = {}\n",
    "\n",
    "for line in open(rootdir + \"/bac175/bac_scaf2bin.txt\").readlines():\n",
    "    \n",
    "    splt = line.strip().split(\"\\t\")\n",
    "    bac_scaf2bin[splt[0]] = splt[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get info for genomes\n",
    "results = {}\n",
    "\n",
    "for result in glob.glob(in_wd + \"/hmm_results/*faa*\"):\n",
    "    hmm = os.path.basename(result).split(\".\")[0]\n",
    "    for record in SeqIO.parse(open(result), \"fasta\"):\n",
    "        m = re.search(\"(\\S+).*\", record.description)\n",
    "        binname = retrieve_bin(m.group(1), bac_scaf2bin)\n",
    "        \n",
    "        if binname not in results:\n",
    "            results[binname] = {hmm: m.group(1)}\n",
    "        else:\n",
    "            results[binname][hmm] = m.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outgroup_df = pd.DataFrame.from_dict(results, orient=\"index\").fillna(\"None\")\n",
    "\n",
    "def count_markers(row,dataset):\n",
    "    count = 0\n",
    "    for hmm in dataset:\n",
    "        if row[hmm] != \"None\":\n",
    "            count += 1\n",
    "    return count\n",
    "        \n",
    "# tabulate marker set completeness for each dataset\n",
    "for dataset, items in tigr_dict.items():\n",
    "    col_name = dataset + \"_count\"\n",
    "    outgroup_df[col_name] = outgroup_df.apply(lambda x: count_markers(x, items.keys()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then filter out incomplete\n",
    "odf = outgroup_df[(outgroup_df[\"rpol_count\"] == 2) & (outgroup_df[\"rp16_count\"] >= 8)]\n",
    "odf[\"name\"] = odf.index\n",
    "\n",
    "# generate cols to keep\n",
    "cols_to_keep = [\"name\", \"rpol_count\", \"rp16_count\"]\n",
    "for dataset, items in tigr_dict.items():\n",
    "    for key in items:\n",
    "        cols_to_keep.append(key)\n",
    "            \n",
    "# merge cpr and outgroup\n",
    "base_sub = base[cols_to_keep]\n",
    "odf_sub = odf[cols_to_keep]\n",
    "merged = pd.concat([base_sub, odf_sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_seq = merged\n",
    "aln_lens = {}\n",
    "\n",
    "def get_sequence(gene, seq_dict):\n",
    "    if gene==\"None\": \n",
    "        return \"None\"\n",
    "    else:\n",
    "        try: return seq_dict[gene]\n",
    "        except: \n",
    "            print(\"%s not found!\" %(gene))\n",
    "            return \"None\"\n",
    "            \n",
    "# add sequences to merged df\n",
    "for trimmed_alignment in glob.glob(out_wd + \"*bmge*\"):\n",
    "    \n",
    "    # first read in trimmed sequences\n",
    "    temp_dict = {}\n",
    "    for record in SeqIO.parse(open(trimmed_alignment, \"r\"), \"fasta\"):\n",
    "        # pull clean headers\n",
    "        m = re.search(\"(\\S+).*\", record.description)\n",
    "        temp_dict[m.group(1)] = str(record.seq)\n",
    "    # now add to the dataframe using apply\n",
    "    hmm = os.path.basename(trimmed_alignment).split(\".\")[0]\n",
    "    col_name = hmm + \"_seq\"\n",
    "    merged_seq[col_name] = merged_seq[hmm].apply(lambda x: get_sequence(x, temp_dict))\n",
    "    # get aln len to use later\n",
    "    aln_lens[hmm] = len(record.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back some metadata\n",
    "base_sub = base[[\"name\", \"final_tax\"]]\n",
    "merged_meta = pd.merge(merged_seq, base_sub, on=\"name\", how=\"left\").fillna(False)\n",
    "# add tax for chloroflexi\n",
    "merged_meta[\"final_tax\"] = merged_meta.apply(lambda x: x[\"final_tax\"] if x[\"final_tax\"] != False else prefix, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process phylogenetic outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually compile bins to keep based on the following criteria - clades > 1 member conserved in both previous concat trees, or known undersampled lineages (ie, Schleper or Torok)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in problematic bins for each tree\n",
    "pbins = {}\n",
    "for file in glob.glob(rootdir + \"/trees/RP*outliers*\"):\n",
    "    print(file)\n",
    "    name = os.path.basename(file).split(\"_\")[0]\n",
    "    pbins[name] = []\n",
    "    for line in open(file).readlines():\n",
    "        pbins[name].append(\"_\".join(line.split(\"_\")[1:]).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outlier(name):\n",
    "    \n",
    "    found = []\n",
    "    for dataset in pbins:\n",
    "        if name in pbins[dataset]:\n",
    "            found.append(dataset)\n",
    "    \n",
    "    if len(found) > 1:\n",
    "        return \"BOTH\"\n",
    "    elif len(found) == 1:\n",
    "        return found[0]\n",
    "    else: return False\n",
    "    \n",
    "# add info in\n",
    "merged_meta[\"outlier\"] = merged_meta.apply(lambda x: get_outlier(x[\"name\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_meta[\"outlier\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run concat trees with outgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_mins = {\"rp16\": 8, \"rpol\": 2}\n",
    "\n",
    "# finally write out the concatenated alignment, pruning previously identified taxa as before\n",
    "for dataset, items in tigr_dict.items():\n",
    "    \n",
    "    filename = out_wd + dataset + \"_concat.\" + prefix + \".pruned.mafft\"\n",
    "    with open(filename, \"w\") as outfile:\n",
    "\n",
    "        # for each genome meeting criteria\n",
    "        for key, row in merged_meta.iterrows():\n",
    "            # write genome if not outlier for this dataset, also meets min gene count threshes\n",
    "            if (row[dataset + \"_count\"] >= count_mins[dataset]) and \\\n",
    "                (row[\"outlier\"] != dataset.upper() and row[\"outlier\"] != \"BOTH\"):\n",
    "                # simplify name ahead of time\n",
    "                seq_name = row[\"final_tax\"] + \"_\" + row[\"name\"]\n",
    "                outfile.write(\">\" + seq_name + \"\\n\")\n",
    "                # now write out sequences\n",
    "                for hmm in items:\n",
    "                    col_name = hmm + \"_seq\"\n",
    "                    # if missing gene, just add gaps\n",
    "                    if row[col_name] == \"None\":\n",
    "                        outfile.write(\"-\"*aln_lens[hmm])\n",
    "                    # if gene present\n",
    "                    else:\n",
    "                        outfile.write(row[col_name])\n",
    "                outfile.write(\"\\n\")\n",
    "            #else:\n",
    "                #print(row[\"name\"], row[\"outlier\"], dataset, row[dataset + \"_count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. Uses qsub, modify below command if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and run the trees\n",
    "for concat_align in glob.glob(out_wd + \"*.pruned.mafft*\"):\n",
    "    \n",
    "    basename = os.path.basename(concat_align).split(\".\")[0] + \".\" + prefix + \".pruned\"\n",
    "    call = \"echo '\" + iqpath + \" -s \" + concat_align + \" -m MFP -st AA -bb 1500 -nt 48 -pre \" + concat_align.split(\".\")[0] + \".\" + prefix + \".final.pruned' | qsub -V -N \" + basename\n",
    "    sp.call(call, shell=True)\n",
    "    #print(call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyze concatenated trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at broader phylogenetic groupings and alpha diversity within groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### taxonomy overrides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in file\n",
    "overrides={}\n",
    "for line in open(rootdir + \"/trees/tax_override.txt\",\"r\").readlines():\n",
    "    genome_name = \"_\".join(line.split(\",\")[0].split(\"_\")[1:])\n",
    "    overrides[genome_name] = line.split(\",\")[1].strip()\n",
    "    \n",
    "# add to base df\n",
    "base[\"revised_tax\"] = base.apply(lambda x: overrides[x[\"name\"]] if x[\"name\"] in overrides else x[\"final_tax\"], axis=1)\n",
    "base[\"override\"] = base.apply(lambda x: x[\"name\"] in overrides, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tree collapsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_tax = {}\n",
    "for key, row in base.iterrows():\n",
    "    revised_tax[row[\"name\"]] = row[\"revised_tax\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_tree(treefile, dictionary):\n",
    "    \n",
    "    # first read in tree\n",
    "    t = Tree(treefile)\n",
    "    # get leaf names\n",
    "    node2labels = t.get_cached_content(store_attr=\"name\")\n",
    "\n",
    "    # define func to assign category\n",
    "    def search_tax(genome):\n",
    "        try: return dictionary[genome]\n",
    "        except: return \"Not found\"\n",
    "    \n",
    "    # define func to identify subtrees\n",
    "    def processable_node(node):\n",
    "        # use post override taxonomies\n",
    "        taxes = [search_tax(\"_\".join(item.split(\"_\")[1:])) for item in list(node2labels[node])]\n",
    "        if len(set(taxes)) == 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # initialize itol outfiles\n",
    "    name = os.path.basename(treefile).split(\".\")[0]\n",
    "    outfile1 = open(treefile.replace(\".treefile\", \".collapse.txt\"), \"w\")\n",
    "    outfile1.write(\"COLLAPSE\\nDATA\\n\")\n",
    "    outfile2 = open(treefile.replace(\".treefile\", \".label.txt\"),\"w\")\n",
    "    outfile2.write(\"DATASET_TEXT\\nSEPARATOR COMMA\\nDATASET_LABEL,labels\\nCOLOR,#ff0000\\nSHOW_INTERNAL,1\\nDATA\\n\")\n",
    "\n",
    "    # iterate through subtrees of same taxonomy\n",
    "    for subtree in t.iter_leaves(is_leaf_fn=processable_node):\n",
    "        to_collapse = []\n",
    "        # collect leaves\n",
    "        for leaf in subtree:\n",
    "            to_collapse.append(leaf.name)\n",
    "        # don't want to collapse singletons\n",
    "        if len(to_collapse) > 1:\n",
    "            \n",
    "            # itol only takes 2 nodes to collapse\n",
    "            distances = []\n",
    "            for leaf in subtree:\n",
    "                for leaf2 in subtree:\n",
    "                    distances.append({\"distance\":t.get_distance(leaf,leaf2), \"nodes\": [leaf.name,leaf2.name]})\n",
    "            # so find the two farthest apart\n",
    "            distances_sorted = sorted(distances, key= lambda x: x[\"distance\"], reverse=True)\n",
    "            tokens = distances_sorted[0][\"nodes\"]\n",
    "            \n",
    "            # now write out to files\n",
    "            clade_name = search_tax(\"_\".join(to_collapse[0].split(\"_\")[1:])) + \" (%d)\" %(len(to_collapse))\n",
    "            #first write collapse file\n",
    "            outfile1.write(\"|\".join(tokens) + \"\\n\")\n",
    "            # then write label file\n",
    "            outfile2.write(\"|\".join(tokens) + \",\" + clade_name + \",-1,#ff0000,bold,2,0\\n\")\n",
    "\n",
    "    outfile1.close()\n",
    "    outfile2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for treefile in glob.glob(rootdir + \"/trees/bac175_outgroup/*final.pruned.treefile*\"):\n",
    "    #print(treefile)\n",
    "    collapse_tree(treefile, revised_tax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### phylogenetic groupings - fig 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeep = [\"Dojkabacteria\", \"WS6\",\"WWE3\", \"Katanobacteria\", \"Saccharibacteria\", \"Howlettbacteria\", \\\n",
    "    \"Berkelbacteria\", \"Kazan\", \"Peregrinibacteria\", \"Gracilibacteria\", \"Absconditabacteria\"]\n",
    "tax2color = {\"Microgenomates\":'#b3e2cd',\"Parcubacteria_1\":'#fdcdac',\"Parcubacteria_2\":'#cbd5e8',\"Parcubacteria_3\":'#f4cae4',\n",
    "             \"Parcubacteria_4\":'#e6f5c9',\"Katanobacteria (WWE3)\":'#fff2ae',\"Saccharibacteria\":'#f1e2cc',\"Peregrinibacteria\":'#cccccc',\"other\":\"white\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create slighly different mapping for downstream\n",
    "mapping = {}\n",
    "for line in open(rootdir + \"/trees/tree_order.txt\").readlines():\n",
    "    phylum, group = line.strip().split(\",\")\n",
    "    if group==\"\":\n",
    "        group = \"other\"\n",
    "    mapping[phylum] = group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"/metabolism/itol/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tree(treefile, dictionary, mode):\n",
    "    \n",
    "    # first read in tree\n",
    "    t = Tree(treefile)\n",
    "    # reroot on bac175\n",
    "    outs = [leaf.name for leaf in t if \"BAC175\" in leaf.name]\n",
    "    root_node = t.get_common_ancestor(outs)\n",
    "    t.set_outgroup(root_node)\n",
    "    # get leaf names\n",
    "    node2labels = t.get_cached_content(store_attr=\"name\")\n",
    "    print(\"Tree read and nodes extracted.\")\n",
    "    \n",
    "    # define func to assign category\n",
    "    def search_tax(genome):\n",
    "        try: return dictionary[genome] if mode == \"div\" else mapping[dictionary[genome]]\n",
    "        except: return \"Not found\"\n",
    "    \n",
    "    # define func to identify subtrees\n",
    "    def processable_node(node):\n",
    "        # use post override taxonomies\n",
    "        taxes = [search_tax(\"_\".join(item.split(\"_\")[1:])) for item in list(node2labels[node])]\n",
    "        if len(set(taxes)) == 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    if mode == \"phy\":\n",
    "        # initialize itol outfiles\n",
    "        name = os.path.basename(treefile).split(\".\")[0]\n",
    "        outfile1 = open(rootdir + \"/metabolism/itol/\" + name + \".allmeta.ranges.txt\", \"w\")\n",
    "        outfile1.write(\"TREE_COLORS\\nSEPARATOR TAB\\nDATA\\n\")\n",
    "        print(\"file initialized...\")\n",
    "    \n",
    "    count = 0\n",
    "    # iterate through subtrees of same taxonomy\n",
    "    for subtree in t.iter_leaves(is_leaf_fn=processable_node):\n",
    "        \n",
    "        to_collapse = []\n",
    "        # collect leaves\n",
    "        for leaf in subtree:\n",
    "            to_collapse.append(leaf.name)\n",
    "        #print(to_collapse)\n",
    "        # don't want to collapse singletons\n",
    "        if len(to_collapse) > 1:\n",
    "            \n",
    "            # itol only takes 2 nodes to collapse\n",
    "            distances = []\n",
    "            for leaf in subtree:\n",
    "                for leaf2 in subtree:\n",
    "                    # don't compare same nodes\n",
    "                    if leaf.name != leaf2.name:\n",
    "                        distances.append({\"distance\":t.get_distance(leaf,leaf2), \"nodes\": [leaf.name,leaf2.name]})\n",
    "            # so find the two farthest apart\n",
    "            distances_sorted = sorted(distances, key= lambda x: x[\"distance\"], reverse=True)\n",
    "            tokens = distances_sorted[0][\"nodes\"]\n",
    "            \n",
    "            # now create output\n",
    "            clade_name = search_tax(\"_\".join(to_collapse[0].split(\"_\")[1:]))\n",
    "            \n",
    "            if mode == \"phy\":\n",
    "                if clade_name != \"other\":\n",
    "                    #first write collapse file\n",
    "                    try: col = tax2color[clade_name]\n",
    "                    except: col = \"white\"\n",
    "                    outfile1.write(\"|\".join(tokens) + \"\\trange\\t\" + col + \"\\t\" + clade_name + \"\\n\")\n",
    "                    # then write label file\n",
    "                    #outfile2.write(\"|\".join(tokens) + \",\" + clade_name + \",-1,#ff0000,bold,2,0\\n\")\n",
    "            count +=1\n",
    "            #print(\"%d subtrees processed...\" %(count))\n",
    "    \n",
    "    if mode == \"phy\":\n",
    "        outfile1.close()\n",
    "        #outfile2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for treefile in glob.glob(rootdir + \"/trees/bac175_outgroup/rp16*final.pruned.treefile\"):\n",
    "    process_tree(treefile, revised_tax, \"phy\")\n",
    "    print(treefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to [top](#Table-of-Contents)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create metabolic annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create metadata and hmm files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in hmm metadata\n",
    "hmm_metadata = pd.read_csv(rootdir + \"/metabolism/hmms/cpr_metabolism_table - hmm_subset_public.tsv\", sep=\"\\t\").fillna(\"none\")\n",
    "# remove parsing error\n",
    "hmm_metadata[\"hmm_name\"] = hmm_metadata[\"hmm_name\"].apply(lambda x: x.replace(\"\\xa0\",\"\"))\n",
    "hmm_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cutoffs(row, cutoff_type):\n",
    "    \n",
    "    cutoff = \"none\"\n",
    "    if row[\"source\"] in [\"Pfam\",\"TIGRFAMs\"]:\n",
    "        # get hmm location\n",
    "        basedir = rootdir + \"/metabolism/hmms/by-source/\" + row[\"source\"] + \"/\"\n",
    "        # extract noise cutoff\n",
    "        for line in open(basedir + row[\"hmm_name\"] + \".hmm\"):\n",
    "            m = re.search(\"^\" + cutoff_type + \"\\s+(\\S+).+\", line)\n",
    "            if m:\n",
    "                cutoff = float(m.group(1))\n",
    "    return cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now fill in noise and trusted cutoffs from hmm files\n",
    "hmm_metadata[\"noise_cutoff\"] = hmm_metadata.apply(lambda x: get_cutoffs(x, \"NC\"), axis=1)\n",
    "hmm_metadata[\"trusted_cutoff\"] = hmm_metadata.apply(lambda x: get_cutoffs(x, \"TC\"), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now actually run hmms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"/metabolism/hmm_results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_metabolic_hmm(hmm_name):\n",
    "    \n",
    "    basename = os.path.basename(hmm_name).replace(\".hmm\", \"\")\n",
    "    result_file = rootdir + \"/metabolism/hmm_results/\" + basename + \".results\"\n",
    "    call = hpath + \" --cpu 10 --tblout \" + result_file + \" \" + hmm_name + \" \" + rootdir + \"/proteins/all_proteins.faa\"\n",
    "    sp.call(call, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 1\n",
    "for source in glob.glob(rootdir + \"metabolism/hmms/by-source/*\"):\n",
    "    for hmm in glob.glob(source + \"/*\"):\n",
    "        run_metabolic_hmm(hmm)\n",
    "        print('hmms run: [%d]\\r'%total, end=\"\")\n",
    "        total += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare file hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create informative file hierarchy for hmm data products\n",
    "basepath = rootdir + \"/metabolism/hmm_data_products/\"\n",
    "\n",
    "if not os.path.exists(basepath):\n",
    "    # create top directory\n",
    "    os.mkdir(basepath)\n",
    "    # then create subdirs for each process\n",
    "    for supergroup in list(set(hmm_metadata[\"supergroup\"])):\n",
    "        os.mkdir(basepath + supergroup.replace(\" \", \"_\") + \"/\")\n",
    "        for group in list(set(hmm_metadata[hmm_metadata[\"supergroup\"]==supergroup][\"group\"])):\n",
    "            scrubbed_name = group.replace(\" \",\"_\").replace(\"/\", \"_\")\n",
    "            os.mkdir(basepath + supergroup.replace(\" \", \"_\") + \"/\" + scrubbed_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create hmm data products for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ IN RESULTS\n",
    "\n",
    "metabolic_hmm_results = {}\n",
    "\n",
    "for hmm_result in glob.glob(rootdir + \"/metabolism/hmm_results/*\"):\n",
    "    metabolic_hmm_results[os.path.basename(hmm_result).split(\".\")[0]] = parse_hmm(hmm_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET OPTIONS\n",
    "sns.set_style(\"ticks\")\n",
    "## open bash wrapper\n",
    "wrapper = open(rootdir + \"/metabolism/hmm_data_products/wrapper.sh\", \"w+\")\n",
    "total=1\n",
    "\n",
    "### CREATE HMM DATA PRODUCTS\n",
    "for index, row in hmm_metadata.iterrows():\n",
    "    \n",
    "    key = row[\"hmm_name\"]\n",
    "    try:\n",
    "        table = metabolic_hmm_results[key]\n",
    "    except: # if no matching hmm\n",
    "        continue\n",
    "    \n",
    "    #only process if there are > x results\n",
    "    if len(table) > 0:\n",
    "        \n",
    "        ## PROCESS TABLE + METADATA\n",
    "        \n",
    "        table[\"position\"] = table.index\n",
    "        # evaluate score and eval\n",
    "        table[\"significant\"] = table[\"eval\"].apply(lambda x: x < 0.05)   \n",
    "        # define outdir for data products\n",
    "        scrubbed_name = row[\"group\"].replace(\" \",\"_\").replace(\"/\", \"_\")\n",
    "        outdir_basename = rootdir + \"/metabolism/hmm_data_products/\" + \\\n",
    "            row[\"supergroup\"].replace(\" \", \"_\") + \"/\" + scrubbed_name + \"/\" + key\n",
    "    \n",
    "        ## FIRST, PLOT\n",
    "        \n",
    "        fig = plt.figure(figsize=(20,4))\n",
    "        # plot trusted and noise cutoffs if applicable\n",
    "        for cutoff in [\"noise_cutoff\", \"trusted_cutoff\"]:\n",
    "            if row[cutoff] != \"none\":\n",
    "                plt.axhline(float(row[cutoff]), ls='--', color=\"grey\")\n",
    "                # position text with slight adjustments\n",
    "                plt.text(int(max(table[\"position\"]))*0.93,float(row[cutoff]) + int(max(table[\"score\"]))*0.025, cutoff + \\\n",
    "                         \" cutoff at \" + str(row[cutoff]), color=\"grey\")\n",
    "        # plot published cutoff if nothing else (custom hmms)\n",
    "        if row[\"noise_cutoff\"] == \"none\" and row[\"trusted_cutoff\"] == \"none\" and row[\"published_cutoff\"] != \"none\":\n",
    "            plt.axhline(float(row[\"published_cutoff\"]), ls='--', color=\"grey\")\n",
    "            plt.text(int(max(table[\"position\"]))*0.93,float(row[\"published_cutoff\"]) + int(max(table[\"score\"]))*0.025, \"published\" + \\\n",
    "                     \" cutoff at \" + str(row[\"published_cutoff\"]), color=\"grey\")\n",
    "\n",
    "        # plot actual data\n",
    "        sns.regplot(\"position\", \"score\", data=table, fit_reg=False, scatter_kws={'s':80}, color=\"blue\")\n",
    "        plt.xlabel(\"rank\")\n",
    "        plt.xticks()\n",
    "        ax2 = plt.twinx()\n",
    "        ax2.set_ylim(0, 0.05)\n",
    "        ax2.grid(False)\n",
    "        sns.regplot(\"position\", \"eval\", data=table, fit_reg=False, scatter_kws={'s':80}, ax=ax2, color=\"orange\")\n",
    "        plt.title(\"%s (%s), published cutoff: %s\" %(row[\"hmm_name\"], row[\"gene\"], row[\"published_cutoff\"]))\n",
    "        plt.savefig(outdir_basename + \".png\", format=\"png\")\n",
    "        # don't display plot\n",
    "        plt.close(fig)\n",
    "        \n",
    "        ## FOR HMMS WITH SIG RESULTS\n",
    "        if list(table[\"significant\"]).count(True) > 0:\n",
    "            \n",
    "            ## PARSE OUT SIGNIFICANT HITS\n",
    "            with open(outdir_basename + \".sighits.txt\",\"w\") as outfile:\n",
    "                for idx,rowe in table[table[\"significant\"]==True].iterrows():\n",
    "                    outfile.write(rowe[\"gene\"] + \"\\n\")\n",
    "            \n",
    "            call = pspath + \" -n \" + outdir_basename + \".sighits.txt -i \" + \\\n",
    "                rootdir + \"/proteins/all_proteins.faa > \" + \\\n",
    "                outdir_basename + \".sighits.faa\"\n",
    "            sp.call(call,shell=True)\n",
    "            \n",
    "            ## NOW ALIGN + TREE BUILD\n",
    "            if list(table[\"significant\"]).count(True) > 1:\n",
    "                call = mpath + \" --thread -10 --reorder \" + outdir_basename + \".sighits.faa > \" + \\\n",
    "                    outdir_basename + \".sighits.mafft\"\n",
    "                wrapper.write(call + \"\\n\")\n",
    "            \n",
    "            if list(table[\"significant\"]).count(True) > 5:\n",
    "                \n",
    "                # BUILD TREE\n",
    "                call = ftpath + \" \" + outdir_basename + \".sighits.mafft > \" + \\\n",
    "                    outdir_basename + \".sighits.tre\"   \n",
    "                wrapper.write(call + \"\\n\")\n",
    "                \n",
    "                # GENERATE ITOL DATA\n",
    "                with open(outdir_basename + \".itol.txt\", \"w+\") as itol:\n",
    "                    itol.write(\"DATASET_SIMPLEBAR\\nSEPARATOR COMMA\\nDATASET_LABEL,hmm_score\\nSHOW_VALUE,1\\nCOLOR,#ff0000\\nDATA\\n\")\n",
    "                    for idx,rowe in table[table[\"significant\"]==True].iterrows():\n",
    "                        itol.write(rowe[\"gene\"] + \",\" + str(rowe[\"score\"]) + \"\\n\")\n",
    "        \n",
    "        print('hmms processed: [%d]\\r'%total, end=\"\")\n",
    "        total += 1\n",
    "\n",
    "wrapper.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Then chmod +x, run 'export OMP_NUM_THREADS=6' for FastTreeMP and run the wrapper manually. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run extra domain hmms for tricky cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = {\"TIGR01418\": [\"PF01326\", \"PF00391\", \"PF02896\"], \"TIGR01251\": [\"PF13793\", \"PF14572\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"/metabolism/extra_markers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpath = rootdir + \"/metabolism/hmm_data_products/\" + \\\n",
    "    \"Higher_C_compounds_metabolism/\"\n",
    "slugs = {\"TIGR01418\": \"Pyruvate_metabolism/\",\"TIGR01251\":\"Pentose_Phosphate_Pathway:_Non-Oxidative_Branch/\"}\n",
    "outpath = rootdir + \"/metabolism/extra_markers/\"\n",
    "\n",
    "for enzyme, pfams in domains.items():\n",
    "    \n",
    "    in_faa = inpath + slugs[enzyme] + enzyme + \".sighits.faa\"\n",
    "    pfam_dfs = {}\n",
    "    \n",
    "    for pfam in pfams:\n",
    "        \n",
    "        #pull down pfam\n",
    "        url = \"https://pfam.xfam.org/family/%s/hmm\" %(pfam)\n",
    "        # name according to pfam, otherwise gene name\n",
    "        outfile = outpath + pfam + \".hmm\"\n",
    "        wget.download(url, out=outfile)\n",
    "        \n",
    "        #run pfam\n",
    "        call = hpath + \" --cut_nc --cpu 6 --tblout \" + outpath + pfam + \".results \" + outpath + pfam + \".hmm \" + in_faa\n",
    "        sp.call(call, shell=True)\n",
    "        \n",
    "        # read in results\n",
    "        result = parse_hmm(outpath + pfam + \".results\")\n",
    "        \n",
    "        # then write out to itol\n",
    "        with open(outpath + pfam + \".extra.itol.txt\", \"w\") as outfile:\n",
    "        \n",
    "            outfile.write(\"DATASET_BINARY\\nSEPARATOR COMMA\\nDATASET_LABEL,\" + pfam + \"\\nCOLOR,#ff0000\\nFIELD_SHAPES,2\\nFIELD_LABELS,\")\n",
    "            outfile.write(pfam + \"\\n\")\n",
    "            outfile.write(\"FIELD_COLORS,#000000\\nDATA\\n\")\n",
    "            \n",
    "            for key, row in result.iterrows():\n",
    "                outfile.write(row[\"gene\"] + \",\" + \"1\" + \"\\n\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter, select, merge results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = {}\n",
    "\n",
    "for index,row in hmm_metadata.iterrows():\n",
    "    if row[\"hmm_name\"] != \"none\":\n",
    "        man = row[\"manual_cutoff\"]\n",
    "        if man != \"none\":\n",
    "            if man == \"noise\":\n",
    "                man = row[\"noise_cutoff\"]\n",
    "            elif man == \"trusted\":\n",
    "                man = row[\"trusted_cutoff\"]\n",
    "            elif man == \"published\":\n",
    "                man = row[\"published_cutoff\"]\n",
    "            elif man == \"all_significant\":\n",
    "                man = 0.05\n",
    "            else: man = float(man)\n",
    "            \n",
    "            cutoffs[row[\"hmm_name\"]] = man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then get filtered results\n",
    "metabolic_hmm_results_filtered = {}\n",
    "\n",
    "for key in metabolic_hmm_results.keys():\n",
    "    # get cutoff - first try new then old\n",
    "    try:\n",
    "        cutoff = cutoffs[key]\n",
    "    except:\n",
    "        print(key + \" not processed.\")\n",
    "        continue\n",
    "    \n",
    "    #print(key, cutoff)\n",
    "    table = metabolic_hmm_results[key]\n",
    "    \n",
    "    # only do if there are results and a cutoff\n",
    "    if len(table) > 0:\n",
    "        # score cutoff (non -inclusive)\n",
    "        if cutoff > 1:\n",
    "            table_filt = table[table[\"score\"] > cutoff]\n",
    "        # eval cutoff (non -inclusive)\n",
    "        else:\n",
    "            table_filt = table[table[\"eval\"] < cutoff]\n",
    "\n",
    "        table_subset = table_filt[[\"gene\", \"score\",\"eval\"]]\n",
    "        table_subset[\"hmm\"] = key\n",
    "        metabolic_hmm_results_filtered[key] = table_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all hmms\n",
    "all_results = pd.concat(list(metabolic_hmm_results_filtered.values()))\n",
    "# then recast as long\n",
    "all_results_long = all_results.pivot(\"gene\", \"hmm\", \"score\").fillna(0)\n",
    "# select best hit per orf\n",
    "all_results_long[\"best_hmm\"] = all_results_long.idxmax(axis=1)\n",
    "all_results_long[\"best_score\"] = all_results_long.max(axis=1)\n",
    "all_results_sub = all_results_long.reset_index()[[\"gene\", \"best_hmm\", \"best_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### then finish analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_sub[\"scaf\"] = all_results_sub[\"gene\"].apply(scaffold)\n",
    "all_results_sub[\"bin\"] = all_results_sub[\"scaf\"].apply(lambda x: retrieve_bin(x, scaf2bin))\n",
    "hmms_df = all_results_sub[[\"bin\", \"gene\", \"best_hmm\", \"best_score\"]]\n",
    "# select best hit for each hmm within a bin\n",
    "hmms_dfp = pd.pivot_table(hmms_df,index=[\"bin\",\"best_hmm\"],columns=\"gene\", values=\"best_score\").fillna(0)\n",
    "hmms_dfp[\"best_gene\"] = hmms_dfp.idxmax(axis=1)\n",
    "hmms_dfp = hmms_dfp.reset_index()[[\"bin\", \"best_hmm\", \"best_gene\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot one more time\n",
    "hmms_final = hmms_dfp.pivot(\"bin\", \"best_hmm\", \"best_gene\").fillna(\"None\")\n",
    "hmms_final = hmms_final.reset_index()\n",
    "# merge\n",
    "base_hmms = pd.merge(base, hmms_final, left_on=\"name\", right_on=\"bin\", how=\"left\").fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns(row, input_cols):\n",
    "    \n",
    "    gene_list = []\n",
    "    \n",
    "    for col in input_cols:\n",
    "        \n",
    "        if row[col] != \"None\":\n",
    "            gene_list.append(row[col])\n",
    "    \n",
    "    # return first hit\n",
    "    if len(gene_list) > 0:\n",
    "        return gene_list[0]\n",
    "    else:\n",
    "        return \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_traits = {}\n",
    "\n",
    "for key, row in pd.DataFrame(hmm_metadata[\"gene\"].value_counts()).iterrows():\n",
    "    \n",
    "    # for traits w/ multi hmms\n",
    "    if row.iloc[0,] > 1:\n",
    "        \n",
    "        # which cols are actually included\n",
    "        cols = set(hmm_metadata[hmm_metadata[\"gene\"]== key][\"hmm_name\"].to_list()).intersection(list(base_hmms.columns))\n",
    "        \n",
    "        # only for those with hits\n",
    "        if len(cols) > 0:\n",
    "            merged_traits[key + \"_merged\"] = list(cols)\n",
    "            base_hmms[key + \"_merged\"] = base_hmms.apply(lambda x: merge_columns(x, list(cols)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### revisualize hmm cutoffs - supp. fig. 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplars = [\"TIGR00419\", \"PF00316\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,row in hmm_metadata.iterrows():\n",
    "        \n",
    "        if row[\"hmm_name\"] in exemplars:\n",
    "            \n",
    "            table = metabolic_hmm_results[row[\"hmm_name\"]]\n",
    "            table[\"position\"] = table.index\n",
    "            table[\"significant\"] = table[\"eval\"].apply(lambda x: x < 0.05)\n",
    "\n",
    "            #THEN PLOT\n",
    "            sns.set(font_scale=1.5)\n",
    "            sns.set_style(\"ticks\")\n",
    "            plt.figure(figsize=(20,6)) \n",
    "            \n",
    "            for cutoff in [\"noise_cutoff\", \"manual_cutoff\"]:\n",
    "                plt.axhline(float(row[cutoff]), ls='--', color=\"grey\")\n",
    "                # position text with slight adjustments\n",
    "                plt.text(int(max(table[\"position\"]))*0.01,float(row[cutoff]) + int(max(table[\"score\"]))*0.02, cutoff.replace(\"_\", \" \") + \\\n",
    "                             \" at \" + str(row[cutoff]), color=\"grey\")\n",
    "                \n",
    "            sns.regplot(\"position\", \"score\", data=table, fit_reg=False, scatter_kws={'s':40}, color=\"blue\")\n",
    "            plt.xlabel(\"rank\")\n",
    "            plt.xticks()\n",
    "            ax2 = plt.twinx()\n",
    "            ax2.set_ylim(0, 0.05)\n",
    "            ax2.grid(False)\n",
    "            sns.regplot(\"position\", \"eval\", data=table, fit_reg=False, scatter_kws={'s':40}, ax=ax2, color=\"orange\")\n",
    "            #plt.title(\"%s\" %(row[\"hmm_name\"]))\n",
    "            outdir = rootdir + \"/figures/\" + row[\"hmm_name\"] + \"_results_annotated\"\n",
    "            plt.savefig(outdir + \".svg\", format=\"svg\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correlating metabolism + phylogeny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic trait distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_traits_values = [item for sublist in merged_traits.values() for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make hmm metadata accessible for sorting and labelling\n",
    "groups = {}\n",
    "for key,row in hmm_metadata.iterrows():\n",
    "    \n",
    "    if row[\"hmm_name\"] in merged_traits_values:\n",
    "        groups[row[\"gene\"] + \"_merged\"] = {\"group\":row[\"supergroup\"], \"process\": row[\"group\"], \"name\": row[\"gene\"]}\n",
    "    else:\n",
    "        groups[row[\"hmm_name\"]] = {\"group\":row[\"supergroup\"], \"process\": row[\"group\"], \"name\": row[\"gene\"]}\n",
    "\n",
    "def get_info(x, datum):\n",
    "    try: return groups[x][datum]\n",
    "    except: return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get traits and sort them\n",
    "traits =[trait for trait in list(set(hmms_df[\"best_hmm\"])) if trait not in merged_traits_values] + list(merged_traits.keys())\n",
    "traits_sorted = sorted(traits, key=lambda x: (get_info(x, \"group\"),get_info(x, \"process\")))\n",
    "# create new name to match tree\n",
    "base_hmms_g = base_hmms[[\"name\",\"final_tax\",\"revised_tax\"] + traits_sorted].drop_duplicates()\n",
    "\n",
    "#reformat dataframe for heatmap\n",
    "base_hmms_clean = base_hmms_g[[\"name\", \"revised_tax\"]]\n",
    "\n",
    "total = 0\n",
    "for trait in traits_sorted:\n",
    "    if len(set(base_hmms_g[trait])) != 1:\n",
    "        base_hmms_clean[trait] = base_hmms_g[trait].apply(lambda x: 1 if x != \"None\" else 0)\n",
    "    \n",
    "    print('traits processed: [%d]\\r'%total,end=\"\")\n",
    "    total += 1\n",
    "\n",
    "#define agg dict\n",
    "agg_dict = {}\n",
    "for trait in traits_sorted:\n",
    "    agg_dict[trait] = \"sum\"\n",
    "agg_dict[\"name\"] = \"count\"\n",
    "\n",
    "#reformat data by phylum\n",
    "base_hmms_gb = base_hmms_clean.groupby(\"revised_tax\",as_index=False).aggregate(agg_dict)\n",
    "# then normalize counts by phylum size\n",
    "for trait in traits_sorted:\n",
    "    base_hmms_gb[trait] = base_hmms_gb.apply(lambda x: x[trait]/float(x[\"name\"]), axis=1)\n",
    "base_hmms_gb.pop(\"name\")\n",
    "base_hmms_gb.columns = [get_info(col, \"name\") for col in base_hmms_gb.columns]\n",
    "base_hmms_gb = base_hmms_gb.set_index(\"revised_tax\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_colors = pd.Series(base_hmms_gb.index).map(mapping).map(tax2color).fillna(\"white\")\n",
    "sns.set(font_scale=1)\n",
    "sns.clustermap(base_hmms_gb,linewidths=1, cmap=\"Blues\", row_cluster=True, figsize=(20,20), row_colors=list(row_colors))\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "#plt.yticks(fontsize=0)\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metabolic profiles through dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with phylum level\n",
    "\n",
    "genome_threshold = 8\n",
    "trait_threshold = 3\n",
    "\n",
    "# try filtering min # of representatives and generics\n",
    "lineages_to_keep = []\n",
    "for phylum in base[\"revised_tax\"].unique():\n",
    "    if list(base[\"revised_tax\"]).count(phylum) > genome_threshold and\\\n",
    "        phylum not in [\"Microgenomates\", \"Parcubacteria\"]:\n",
    "        lineages_to_keep.append(phylum)\n",
    "\n",
    "# generate trait counts\n",
    "traits_to_keep = []\n",
    "for col in list(base_hmms_clean.columns):\n",
    "    if col not in [\"name\", \"revised_tax\"]:\n",
    "        if base_hmms_clean[col].sum() > trait_threshold:\n",
    "            traits_to_keep.append(col)\n",
    "\n",
    "print(len(lineages_to_keep), len(traits_to_keep))\n",
    "\n",
    "#define agg dict\n",
    "agg_dict = {}\n",
    "for trait in traits_to_keep:\n",
    "    agg_dict[trait] = \"sum\"\n",
    "agg_dict[\"name\"] = \"count\"\n",
    "\n",
    "# regenerate base_hmms_gb applying filters\n",
    "filtered = base_hmms_clean[base_hmms_clean[\"revised_tax\"].isin(lineages_to_keep)][[\"name\", \"revised_tax\"] + traits_to_keep]\n",
    "fgb = filtered.groupby(\"revised_tax\",as_index=False).aggregate(agg_dict)\n",
    "for trait in traits_to_keep:\n",
    "    fgb[trait] = fgb.apply(lambda x: x[trait]/float(x[\"name\"]), axis=1)\n",
    "fgb.pop(\"name\")\n",
    "fgb.columns = [get_info(col, \"name\") for col in fgb.columns]\n",
    "fgb = fgb.set_index(\"revised_tax\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate distance matrix\n",
    "brayD = ep.distance(fgb, method='bray', transform='1')\n",
    "# then use skbio to do pcoA\n",
    "results = pcoa(brayD)\n",
    "results.proportion_explained.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add metadata back to results and custom plot\n",
    "pc_results = pd.DataFrame(results.samples)\n",
    "pc_results[\"name\"] = fgb.index\n",
    "pc_results[\"group\"] = pc_results[\"name\"].apply(lambda x: mapping[x])\n",
    "pc_results[\"lineage_size\"] = pc_results[\"name\"].apply(lambda x: list(base[\"revised_tax\"]).count(x))\n",
    "\n",
    "sns.set(font_scale=1.25)\n",
    "sns.set_style(\"ticks\")\n",
    "#sns.lmplot(\"PC1\", \"PC2\", data=pc_results, hue=\"group\", \\\n",
    "    #scatter_kws={\"s\":100, \"linewidth\":0.5, \"edgecolors\":\"grey\", \"alpha\":1}, fit_reg=False, size=7, aspect=1.5, palette=tax2color, legend=False)\n",
    "kws = dict(linewidth=.5, edgecolor=\"black\")\n",
    "sns.relplot(\"PC2\", \"PC1\", data=pc_results, size=\"lineage_size\", hue=\"group\", \\\n",
    "    palette=tax2color, height=10, aspect=.9, sizes=(50,400),**kws)\n",
    "\n",
    "for line in range(0,pc_results.shape[0]):\n",
    "    if (list(pc_results[\"PC1\"])[line] > -10) or (list(pc_results[\"PC2\"])[line] < -0.12) or (list(pc_results[\"PC2\"])[line] > 0.0):\n",
    "        plt.text(list(pc_results[\"PC2\"])[line]+0.008, list(pc_results[\"PC1\"])[line]-0.003, str(list(pc_results[\"name\"])[line]), horizontalalignment='left',size=10, color='grey')\n",
    "            \n",
    "#plt.legend(loc=\"lower right\")\n",
    "plt.ylabel(\"PC1 (%s variance)\" %(results.proportion_explained[\"PC1\"]).round(2))\n",
    "plt.xlabel(\"PC2 (%s variance)\" %(results.proportion_explained[\"PC2\"]).round(2))\n",
    "#plt.yscale(\"log\")\n",
    "plt.savefig(rootdir + \"/figures/lineage_clustering.svg\", format=\"svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building a metabolic reference set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract a well-sampled set of reference sequences for metabolisms of interest from BAC175."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"/bac175/metabolic_hmm_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_metabolic_hmm(hmm_name):\n",
    "    \n",
    "    basename = os.path.basename(hmm_name).replace(\".hmm\", \"\")\n",
    "    result_file = rootdir + \"/bac175/metabolic_hmm_results/\" + basename + \".results\"\n",
    "    call = \"hmmsearch --cpu 10 --tblout \" + result_file + \" \" + hmm_name + \" \" + rootdir + \"/bac175/Bacteria175.cleaned.faa\"\n",
    "    sp.call(call, shell=True)\n",
    "    #print(call)\n",
    "\n",
    "total = 1\n",
    "for source in glob.glob(rootdir + \"/metabolism/hmms/by-source/*\"):\n",
    "    for hmm in glob.glob(source + \"/*\"):\n",
    "        # only run for traits of interest\n",
    "        ref_traits = traits + merged_traits_values\n",
    "        if os.path.basename(hmm).split(\".\")[0] in ref_traits:\n",
    "            run_metabolic_hmm(hmm)\n",
    "            print('hmms run: [%d]\\r'%total, end=\"\")\n",
    "            total += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these genomes, just use noise cutoff or published cutoff (automated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = {}\n",
    "\n",
    "for key, row in hmm_metadata.iterrows():\n",
    "    \n",
    "    if row[\"noise_cutoff\"] != \"none\":\n",
    "        cutoff = row[\"noise_cutoff\"]\n",
    "    elif row[\"published_cutoff\"] != \"none\":\n",
    "        cutoff = row[\"published_cutoff\"]\n",
    "    else:\n",
    "        cutoff = row[\"manual_cutoff\"]\n",
    "    cutoffs[row[\"hmm_name\"]] = cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ IN RESULTS\n",
    "\n",
    "ref_metabolic_hmm_results = {}\n",
    "\n",
    "for hmm_result in glob.glob(rootdir + \"/bac175/metabolic_hmm_results/*\"):\n",
    "    ref_metabolic_hmm_results[os.path.basename(hmm_result).split(\".\")[0]] = parse_hmm(hmm_result)\n",
    "    \n",
    "# then get filtered results\n",
    "ref_metabolic_hmm_results_filtered = {}\n",
    "\n",
    "for key in ref_metabolic_hmm_results.keys():\n",
    "    \n",
    "    try:\n",
    "        cutoff = cutoffs[key]\n",
    "    except:\n",
    "        continue\n",
    "    #print(key, cutoff)\n",
    "    table = ref_metabolic_hmm_results[key]\n",
    "    #table[\"position\"] = table.index\n",
    "        \n",
    "    # only do if there are results and a cutoff\n",
    "    if len(table) > 0:\n",
    "        table_filt = table[table[\"score\"] > float(cutoff)]\n",
    "        table_subset = table_filt[[\"gene\", \"score\",\"eval\"]]\n",
    "        table_subset[\"hmm\"] = key\n",
    "        ref_metabolic_hmm_results_filtered[key] = table_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all hmms\n",
    "all_results = pd.concat(list(ref_metabolic_hmm_results_filtered.values()))\n",
    "# then recast as long\n",
    "all_results_long = all_results.pivot(\"gene\", \"hmm\", \"score\").fillna(0)\n",
    "# select best hit\n",
    "all_results_long[\"best_hmm\"] = all_results_long.idxmax(axis=1)\n",
    "all_results_long[\"best_score\"] = all_results_long.max(axis=1)\n",
    "all_results_sub = all_results_long.reset_index()[[\"gene\", \"best_hmm\", \"best_score\"]]\n",
    "\n",
    "### then finish analysis\n",
    "\n",
    "all_results_sub[\"name\"] = all_results_sub[\"gene\"].apply(lambda x: x.split(\"@\")[0])\n",
    "hmms_df = all_results_sub[[\"name\", \"gene\", \"best_hmm\", \"best_score\"]]\n",
    "# select best hit for each hmm within a bin\n",
    "hmms_dfp = pd.pivot_table(hmms_df,index=[\"name\",\"best_hmm\"],columns=\"gene\", values=\"best_score\").fillna(0)\n",
    "hmms_dfp[\"best_gene\"] = hmms_dfp.idxmax(axis=1)\n",
    "hmms_dfp = hmms_dfp.reset_index()[[\"name\", \"best_hmm\", \"best_gene\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot one more time\n",
    "hmms_final = hmms_dfp.pivot(\"name\", \"best_hmm\", \"best_gene\").fillna(\"None\")\n",
    "hmms_final = hmms_final.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genomes_metab = pd.concat([base_hmms_g, hmms_final]).fillna(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fig. 1a - metabolisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#d9d9d9','#1f78b4','#6a3d9a','#33a02c','#a6cee3','#fb9a99','#ff7f00','#fdbf6f','#cab2d6','#b15928']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute hmm wise hit counts\n",
    "sub = all_genomes_metab[~all_genomes_metab[\"name\"].str.contains(\"BAC175\")]\n",
    "hit_counts = {}\n",
    "for hmm in list(sub.columns):\n",
    "    hits = 0\n",
    "    for key,row in sub.iterrows():\n",
    "        if row[hmm] != \"None\":\n",
    "            hits += 1\n",
    "    hit_counts[hmm] = hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_to_remove = [\"Reverse TCA cycle\", \"Reverse TCA cycle\", \"Metal (Iron/Manganese) oxidation/reduction\", \"Gluconeogenesis\",\n",
    "                   \"Arsenate reduction\", \"Methanol oxidation\", \"Sulfur oxidation\", \"Formaldehyde oxidation\", \"TCA cycle\", \"Nitric oxide reduction\",\n",
    "                   \"FeFe hydrogenase\", \"CO oxidation\", \"Halogenated compounds breakdown\",\"N2 fixation\", \"Sulfate reduction\", \"Sulfite reduction\",\n",
    "                   \"Oxygen metabolism - cytochrome (quinone) oxidase, bd type\"]\n",
    "\n",
    "group_order={\"Glycolysis\": 1, \"Pentose Phosphate Pathway: Non-Oxidative Branch\": 2, \"Pentose Phosphate Pathway: Oxidative Branch\":3,\n",
    "            \"Pyruvate metabolism\": 4, \"Acetate/Lactate Metabolism\": 5, \"Nucleotide salvage pathway\":6, \"Other\": 7, \"Hydrogen/Sulfur Metabolism\":8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metab_colors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify hmm_metadata for merged\n",
    "mod_meta = hmm_metadata\n",
    "\n",
    "for key in merged_traits:\n",
    "    for hmm in merged_traits[key]:\n",
    "        mod_meta = mod_meta.replace(hmm, key)\n",
    "\n",
    "mod_meta_sub = mod_meta[[\"group\", \"gene\", \"hmm_name\"]]\n",
    "mod_meta_sub2 = mod_meta_sub[mod_meta_sub[\"hmm_name\"].isin(traits)].drop_duplicates()     \n",
    "mmf = mod_meta_sub2[~mod_meta_sub2[\"group\"].isin(groups_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassign groups\n",
    "fixes = {\"Fermentation\": \"Acetate/Lactate Metabolism\", \"Acetate metabolism\": \"Acetate/Lactate Metabolism\", \"Ni-Fe Hydrogenase\": \"Other\", \"Nitrite reduction\": \"Other\", \"Oxygen metabolism - cytochrome (quinone) oxidase, bo type\": \"Other\"}\n",
    "mmf[\"group\"] = mmf[\"group\"].apply(lambda x: x if x not in fixes else fixes[x])\n",
    "# get hit counts\n",
    "mmf[\"count\"] = mmf[\"hmm_name\"].apply(lambda x: hit_counts[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make hmm metadata dict\n",
    "hmm_dict = {}\n",
    "\n",
    "for index, row in mmf.iterrows():\n",
    "    hmm_dict[row[\"hmm_name\"]] = {\"gene\": row[\"gene\"], \"group\": row[\"group\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(rootdir + \"/metabolism/itol/all_metabs.txt\",\"w\") as outfile:\n",
    "    \n",
    "    groupss = list(set(mmf[\"group\"]))\n",
    "    outfile.write(\"DATASET_BINARY\\nSEPARATOR COMMA\\nDATASET_LABEL,all_metabolism\\nCOLOR,#ff0000\\nFIELD_SHAPES,\" + \\\n",
    "        (\"1,\"*(len(mmf) + len(groupss)-1)).strip(\",\") + \"\\nFIELD_LABELS,\")\n",
    "    \n",
    "    #print((len(hmm_subset) + len(groups)))\n",
    "    field_labels = \"\"\n",
    "    color_string = \"\"\n",
    "    \n",
    "    # get hit counts for each hmm\n",
    "    test = mmf[[\"group\", \"count\"]]\n",
    "    #compute relative completeness\n",
    "    test[\"frac\"] = test[\"count\"].apply(lambda x: x/float(max(mmf[\"count\"])))\n",
    "    tgb = test.groupby(\"group\",as_index=False).aggregate({\"frac\":\"mean\"})\n",
    "    hmm_sub2 = mmf.merge(tgb, on=\"group\")\n",
    "    hmm_sub2[\"order\"] = hmm_sub2[\"group\"].map(group_order)\n",
    "    # get a sorted df\n",
    "    sorted_df = hmm_sub2.sort_values([\"order\", \"count\"], ascending=[True,False])\n",
    "    \n",
    "    position = \"None\"\n",
    "    for key, row in sorted_df.iterrows():\n",
    "        if position != row[\"group\"] and position != \"None\":\n",
    "            field_labels += \",\"\n",
    "            color_string += \"#ffffff,\"\n",
    "        field_labels += hmm_dict[row[\"hmm_name\"]][\"gene\"]+\",\"\n",
    "        color_string += colors[row[\"order\"] % len(colors)] + \",\"\n",
    "        position = row[\"group\"]\n",
    "        metab_colors[row[\"group\"]] = colors[row[\"order\"] % len(colors)]\n",
    "        #print(row[\"group\"], row[\"order\"], colors[row[\"order\"] % len(colors)])\n",
    "    \n",
    "    #print(len(field_labels.split(\",\")), len(color_string.split(\",\")))\n",
    " \n",
    "    # write out blocks, removing final comma where necessary\n",
    "    outfile.write(field_labels.strip(\",\") + \"\\n\")\n",
    "    outfile.write(\"FIELD_COLORS,\" + color_string.strip(\",\") + \"\\nDATA\\n\")\n",
    "    \n",
    "    # now go by genome\n",
    "    for key, row in all_genomes_metab.iterrows():\n",
    "        if \"BAC175\" in row[\"name\"]:\n",
    "            outfile.write(row[\"name\"] + \",\")\n",
    "        else:\n",
    "            # instead of revised to match old names\n",
    "            outfile.write(row[\"final_tax\"] + \"_\" + row[\"name\"] + \",\")\n",
    "        data_block = \"\"\n",
    "        # iterate through metabs in order\n",
    "        position = \"None\"\n",
    "        for keye, rowe in sorted_df.iterrows():\n",
    "            if position != rowe[\"group\"] and position != \"None\":\n",
    "                data_block += \"-1,\"\n",
    "            data_block += \"1,\" if row[rowe[\"hmm_name\"]] != \"None\" else \"-1,\"\n",
    "            position = rowe[\"group\"]\n",
    "        outfile.write(data_block.strip(\",\") + \"\\n\")\n",
    "        #print(len(data_block.split(\",\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trait depth + homoplasy metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create input for R script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"/metabolism/trait_analysis/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteratively write out binary matrix for all traits\n",
    "base_hmms = base_hmms.drop_duplicates()\n",
    "base_hmms[\"newname\"] = base_hmms.apply(lambda x: x[\"final_tax\"] + \"_\" + x[\"name\"], axis=1)\n",
    "base_hmms_sub = base_hmms[[\"newname\"] + traits]\n",
    "\n",
    "base_hmms_clean = base_hmms_sub[[\"newname\"]]\n",
    "\n",
    "total = 0\n",
    "for trait in traits_sorted:\n",
    "    if len(set(base_hmms_sub[trait])) != 1:\n",
    "        base_hmms_clean[trait] = base_hmms_sub[trait].apply(lambda x: 1 if x != \"None\" else 0)\n",
    "    \n",
    "    print('traits processed: [%d]\\r'%total,end=\"\")\n",
    "    total += 1\n",
    "\n",
    "# add in dummy outgroup data\n",
    "t = Tree(rootdir + \"/trees/bac175_outgroup/rp16_concat.BAC175.final.pruned.treefile\")\n",
    "outs = [leaf.name for leaf in t if \"BAC175\" in leaf.name]\n",
    "top_dict = {}\n",
    "for item in outs:\n",
    "    temp_dict = {}\n",
    "    for trait in traits:\n",
    "        temp_dict[trait] = 0\n",
    "    top_dict[item] = temp_dict\n",
    "\n",
    "outdf = pd.DataFrame.from_dict(top_dict, orient=\"index\")\n",
    "outdf = outdf.reset_index()\n",
    "outdf.columns = [\"newname\"] + traits\n",
    "merged = pd.concat([base_hmms_clean.drop_duplicates(), outdf])\n",
    "\n",
    "# write out for R\n",
    "merged.to_csv(rootdir + \"/metabolism/trait_analysis/trait_table.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associated viz for itol\n",
    "for trait in traits:\n",
    "    \n",
    "    if (\"TIGR\") in trait or (\"Pfam\" in trait):\n",
    "        out_path = groups[trait][\"name\"].replace(\" \", \"_\").replace(\"/\",\"_\") + \"_\" + trait + \".txt\"\n",
    "    else: out_path = trait.replace(\"/\",\"_\") + \".txt\"\n",
    "    \n",
    "    with open(rootdir + \"/metabolism/itol/\" + out_path,\"w\") as outfile:\n",
    "        outfile.write(\"TREE_COLORS\\nSEPARATOR TAB\\nDATA\\n\")\n",
    "        for key, row in base_hmms_clean.iterrows():\n",
    "            if row[trait] != 0:\n",
    "                outfile.write(row[\"newname\"]+ \"\\trange\\t\" + \"#b3e2cd\" + \"\\t\" + trait + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: RUN R SCRIPT cpr-phylo-R-public.R to generate results used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = pd.read_csv(rootdir + \"/metabolism/trait_analysis/bac175_consentrait_results.csv\")\n",
    "td[\"group\"] = td[\"trait\"].apply(lambda x: groups[x.replace(\".\", \"/\")][\"process\"])\n",
    "td[\"gene\"] = td[\"trait\"].apply(lambda x: groups[x.replace(\".\", \"/\")][\"name\"])\n",
    "tdm = td[[\"tree\",\"trait\",\"group\", \"gene\", \"min_fraction\", \"pval\", \"clade\", \"mean_depth\"]]\n",
    "tdm[\"desc\"] = tdm.apply(lambda x: x[\"trait\"] + \" - \" + x[\"gene\"], axis=1)\n",
    "tdm[\"sig\"] = tdm[\"pval\"].apply(lambda x: x < 0.05)\n",
    "# mean sort for boxplot\n",
    "order = list(tdm.groupby(\"trait\").aggregate({\"mean_depth\": \"median\"}).sort_values(\"mean_depth\", ascending=False).reset_index()[\"trait\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets focus on rp 16 at a certain threshold\n",
    "thresh = 0.9\n",
    "sub = tdm[(tdm[\"min_fraction\"]==thresh) & (tdm[\"tree\"]==\"rp16\")]\n",
    "order = list(sub.groupby(\"desc\").aggregate({\"mean_depth\": \"median\"}).sort_values(\"mean_depth\", ascending=False).reset_index()[\"desc\"])\n",
    "plt.figure(figsize=(30,8))\n",
    "sns.set(font_scale=1.75)\n",
    "sns.set_style(\"white\")\n",
    "sns.boxplot(\"desc\", \"mean_depth\", data=sub, palette={True:\"red\", False:\"pink\"}, order=order, linewidth=0.5, hue=\"sig\")\n",
    "sns.stripplot(\"desc\", \"mean_depth\", data=sub, jitter=True, size=5, linewidth=1, order=order, color=\"black\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"phylogenetic depth of positive clades, by trait (min_fraction = %.2f)\" %(thresh))\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"phylogenetic depth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process homoplasy info\n",
    "hm = pd.read_csv(rootdir + \"/metabolism/trait_analysis/bac175_ci_results.csv\")\n",
    "# using formula from annotree paper\n",
    "hm[\"patchiness\"] = hm.apply(lambda x: math.log(x[\"ci\"])/float(math.log(x[\"family_size\"])) \\\n",
    "    if x[\"family_size\"]>1 else None, axis=1)\n",
    "# filter out small fam sizes\n",
    "#hm = hm[hm[\"family_size\"]>3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set display classes\n",
    "display_groups = list(group_order.keys()) + [\"Ni-Fe Hydrogenase\", \"Nitrite reduction\", \"Oxygen metabolism - cytochrome (quinone) oxidase, bo type\"]\n",
    "# add colors\n",
    "new_cols = ['#fdbf6f','#cab2d6','#b15928']\n",
    "\n",
    "count=0\n",
    "for metab in display_groups:\n",
    "    \n",
    "    if metab not in list(group_order.keys()):\n",
    "        \n",
    "        metab_colors[metab] = new_cols[count]\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot td vs homoplasy for one min_frac\n",
    "td_sub = td[td[\"min_fraction\"]==thresh].groupby([\"tree\", \"trait\"], as_index=False).aggregate({\"mean_depth\":\"mean\"})\n",
    "both = pd.merge(td_sub, hm, on=[\"tree\",\"trait\"])\n",
    "both[\"group\"] = both[\"trait\"].apply(lambda x: groups[x.replace(\".\", \"/\")][\"process\"])\n",
    "both[\"group\"] = both[\"group\"].apply(lambda x: x if x not in [\"Acetate metabolism\", \"Fermentation\"] else \"Acetate/Lactate Metabolism\")\n",
    "#both_meta = pd.merge(both, hmm_metadata, left_on=\"trait\", right_on=\"hmm_name\")\n",
    "# filter down to metabs of interest\n",
    "both_meta_sub = both[both[\"group\"].isin(display_groups)]\n",
    "for tree in both[\"tree\"].unique():\n",
    "    \n",
    "    sns.set(font_scale=1)\n",
    "    sns.set_style(\"ticks\")\n",
    "    kws = dict(linewidth=.5, edgecolor=\"black\")\n",
    "    sns.relplot(\"mean_depth\", \"patchiness\", data=both_meta_sub[both_meta_sub[\"tree\"]==tree], size=\"family_size\", hue=\"group\", \\\n",
    "                palette=metab_colors, alpha=0.8, height=6, aspect=1.1, sizes=(50,400), **kws, legend=\"brief\")\n",
    "    #l = plt.legend(loc=\"best\")\n",
    "    #l.set_title('Metabolism')\n",
    "    # taking out one enzyme\n",
    "    #plt.xlim([0,0.7])\n",
    "    #plt.xscale(\"log\")\n",
    "    plt.xlabel(\"mean phylogenetic depth\")\n",
    "    #plt.title(\"evolutionary profiles on \" + tree + \" tree\")\n",
    "    if tree == \"rp16\":\n",
    "        plt.savefig(rootdir + \"/figures/all_evol_profiles.svg\", format=\"svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in both_meta_sub[\"group\"].unique():\n",
    "    \n",
    "    if group in [\"Glycolysis\"]:\n",
    "        subset = both_meta_sub[(both_meta_sub[\"tree\"]==\"rp16\")]\n",
    "        subset[\"in_gene_set\"] = both_meta_sub[\"group\"].apply(lambda x: x == group)\n",
    "        sns.set(font_scale=1)\n",
    "        sns.set_style(\"ticks\")\n",
    "        kws = dict(linewidth=.5, edgecolor=\"black\")\n",
    "        sns.relplot(\"mean_depth\", \"patchiness\", data=subset, size=\"family_size\", hue=\"in_gene_set\", \\\n",
    "                    palette={True:metab_colors[group], False:\"white\"}, alpha=0.8, height=6, aspect=1.1, sizes=(50,400), **kws, legend=\"brief\")\n",
    "        #l = plt.legend(loc=\"best\")\n",
    "        #l.set_title('Metabolism')\n",
    "        # taking out one enzyme\n",
    "        #plt.xlim([0,0.7])\n",
    "        #plt.xscale(\"log\")\n",
    "        plt.xlabel(\"mean phylogenetic depth\")\n",
    "        plt.savefig(rootdir + \"/figures/glycolysis_evol_profiles.svg\", format=\"svg\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create gene trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pull seqs, align, treebuild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define traits to focus on\n",
    "traits_sub = hmm_metadata[(hmm_metadata[\"group\"].str.contains(\"Glycolysis\"))][\"hmm_name\"].to_list()\n",
    "# traits for which we want archaeal references added\n",
    "arch_traits = [\"PF06560\", \"TIGR02128\", \"TIGR00306\", \"TIGR00419\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = rootdir + \"/metabolism/gene_trees/filtered_sequences/\"\n",
    "\n",
    "cmdir(rootdir + \"/metabolism/gene_trees/\")\n",
    "cmdir(basedir)\n",
    "\n",
    "def get_arch_refs(trait):\n",
    "    \n",
    "    # run hmmsearch w/ threshold\n",
    "    db = \"TIGRFAMs\" if \"TIGR\" in trait else \"Pfam\"\n",
    "    call = hpath + \" --cut_nc --cpu 6 --tblout \" + basedir + trait + \".ARC.results \" + \\\n",
    "        rootdir + \"/metabolism/hmms/by-source/\" + db + \"/\" + trait + \".hmm \" + \\\n",
    "        rootdir + \"/metabolism/Archaea300.mod.faa\"\n",
    "    sp.call(call, shell=True)\n",
    "    \n",
    "    # read in results + write out names\n",
    "    with open(basedir + trait + \".ARC.names\", \"w\") as outfile:\n",
    "        for key, row in parse_hmm(basedir + trait + \".ARC.results\").iterrows():\n",
    "            outfile.write(row[\"gene\"] + \"\\n\")\n",
    "    \n",
    "def extract_pull(trait, basedir, archaeal):\n",
    "    \n",
    "    gene_list = [gene for gene in base_hmms[trait] if gene != \"None\"] + \\\n",
    "        [gene for gene in hmms_final[trait] if gene != \"None\"]\n",
    "    \n",
    "    with open(basedir + trait + \".names\", \"w\") as q_out:\n",
    "        for gene in gene_list:\n",
    "            q_out.write(gene + \"\\n\")\n",
    "\n",
    "    call = pspath + \" -i \" + rootdir + \"/proteins/all_proteins.faa\" + \\\n",
    "        \" -n \" + basedir + trait + \".names >> \" + basedir + trait + \".concat.faa\"\n",
    "    call2 = pspath + \" -i \" + rootdir + \"/bac175/Bacteria175.mod.faa\" + \\\n",
    "        \" -n \" + basedir + trait + \".names >> \" + basedir + trait + \".concat.faa\"\n",
    "    \n",
    "    sp.call(call,shell=True)\n",
    "    sp.call(call2, shell=True)\n",
    "    \n",
    "    if archaeal == True:\n",
    "        \n",
    "        # create reference sequences\n",
    "        get_arch_refs(trait)\n",
    "        # add to concat file\n",
    "        call3 = pspath + \" -i \" + rootdir + \"metabolism/Archaea300.mod.faa\" + \\\n",
    "            \" -n \" + basedir + trait + \".ARC.names >> \" + basedir + trait + \".concat.faa\"\n",
    "        sp.call(call3, shell=True)\n",
    "        \n",
    "with open(basedir + \"wrapper.sh\", \"w\") as wrapper:\n",
    "\n",
    "    for trait in traits_sub:\n",
    "        \n",
    "        # only for those w/ valid hits\n",
    "        if trait in base_hmms.columns.to_list() and trait in hmms_final.columns.to_list():\n",
    "            \n",
    "            if trait in arch_traits:\n",
    "                extract_pull(trait, basedir, True)\n",
    "            \n",
    "            else:\n",
    "                extract_pull(trait, basedir, False)\n",
    "\n",
    "            # write out align + tree calls to wrapper\n",
    "            aln_call = mpath + \" --thread 8 --anysymbol --reorder \" + basedir + trait + \".concat.faa > \" + basedir + trait + \".concat.mafft\"\n",
    "            wrapper.write(aln_call + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Then chmod +x and run the wrapper manually. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use Geneious or other method to strip columns with 95% or more gaps in each single gene alignment, creating *.stripped.mafft* versions for each alignment used in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run trees on iqtree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. Uses qsub, modify below command if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and run the trees\n",
    "for aln in glob.glob(rootdir + \"/metabolism/gene_trees/filtered_sequences/*concat*stripped.mafft\"):\n",
    "    \n",
    "    basename = os.path.basename(aln).split(\".\")[0]\n",
    "    call = \"echo '\" + iqpath + \" -s \" + aln + \" -m TEST -st AA -bb 1500 -nt 48 -pre \" + aln.split(\".\")[0] + \"' | qsub -V -N \" + basename\n",
    "    sp.call(call, shell=True)\n",
    "    #print(call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create mod trees + itol files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_tax = {}\n",
    "for key, row in base.iterrows():\n",
    "    revised_tax[row[\"name\"]] = row[\"revised_tax\"]\n",
    "    \n",
    "# add taxes not already present\n",
    "for cat in base[\"revised_tax\"].unique():\n",
    "    if cat not in mapping:\n",
    "        mapping[cat] = \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"/metabolism/gene_trees/itol/\")\n",
    "cmdir(rootdir + \"/metabolism/gene_trees/mod_trees/\")\n",
    "\n",
    "for tree in glob.glob(rootdir + \"/metabolism/gene_trees/filtered_sequences/*treefile\"):\n",
    "    \n",
    "    basename = os.path.basename(tree).split(\".\")[0]\n",
    "    t = Tree(tree)\n",
    "    with open(rootdir + \"/metabolism/gene_trees/itol/\" + basename + \".itol.txt\", \"w\") as outfile:\n",
    "        outfile.write(\"TREE_COLORS\\nSEPARATOR TAB\\nDATA\\n\")\n",
    "        for leaf in t:\n",
    "            # remove geneious artifact\n",
    "            leaf.name = leaf.name.replace(\"__stripped_\",\"\")\n",
    "            if \"BAC175\" in leaf.name:\n",
    "                outfile.write(leaf.name + \"\\trange\\t#595957\\t\" + basename + \"\\n\")\n",
    "            elif \"ARC\" in leaf.name:\n",
    "                outfile.write(leaf.name + \"\\trange\\t#1c1c1b\\t\" + basename + \"\\n\")\n",
    "            else:\n",
    "                #print(leaf.name)\n",
    "                bin = retrieve_bin(scaffold(leaf.name), scaf2bin)\n",
    "                #print(bin)\n",
    "                leaf.name = bin + \"_\" + leaf.name\n",
    "                outfile.write(leaf.name + \"\\trange\\t\" + tax2color[mapping[revised_tax[bin]]] + \"\\t\" + basename + \"\\n\")\n",
    "        t.write(outfile=rootdir + \"/metabolism/gene_trees/mod_trees/\" + basename + \".mod.tre\",format=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metabolic case studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hydrogenase 3b tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a trimmed alignment of CPR hydrogenase plus a dereplicated reference set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify alignment file\n",
    "with open(rootdir + \"/metabolism/case_studies/hyd.concat.cleaned.mafft\", \"w\") as clean_aln:\n",
    "    \n",
    "    for record in SeqIO.parse(open(rootdir + \"/metabolism/case_studies/hyd.concat.stripped.mafft\"), \"fasta\"):\n",
    "        \n",
    "        if \":\" in record.description:\n",
    "            new_desc = record.description.replace(\":\", \"_\")\n",
    "        else:\n",
    "            new_desc = record.description\n",
    "        \n",
    "        clean_aln.write(\">\" + new_desc + \"\\n\" + str(record.seq) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and run the trees\n",
    "for aln in glob.glob(outdir + \"/hyd.concat.cleaned.mafft\"):\n",
    "    \n",
    "    basename = os.path.basename(aln).split(\".\")[0]\n",
    "    call = iqpath + \" -s \" + aln + \" -m TEST -st AA -bb 1500 -nt 6 -pre \" + aln.split(\".\")[0] + \".concat.cleaned.\"\n",
    "    sp.call(call, shell=True)\n",
    "    #print(call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_many_colors = [\"#e6194B\",\"#3cb44b\",\"#ffe119\",\"#4363d8\",\"#f58231\",\"#911eb4\",\"#ffa500\",\"#FFD280\",\n",
    "                  \"#42d4f4\",\"#f032e6\",\"#bfef45\",\"#fabebe\",\"#469990\",\"#e6beff\",\"#9A6324\",\"#fffac8\",\"#800000\",\"#aaffc3\",\"#808000\",\"#ffd8b1\",\n",
    "                  \"#000075\",\"#a9a9a9\",\"#ffffff\",\"#000000\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tree(rootdir + \"/metabolism/case_studies/hyd.concat.cleaned.tre\")\n",
    "\n",
    "groups = []\n",
    "\n",
    "with open(rootdir + \"/metabolism/case_studies/sulf_forms.itol.txt\",\"w\") as outfile:\n",
    "    outfile.write(\"TREE_COLORS\\nSEPARATOR TAB\\nDATA\\n\")\n",
    "    for leaf in t:\n",
    "        if \"Form_V\" in leaf.name:\n",
    "            name = leaf.name.replace(\"Form_V\", \"Group_1h/5\")\n",
    "        else:\n",
    "            name = leaf.name\n",
    "        if \"Group\" in name:\n",
    "            group = \" \".join(name.split(\"_\")[-2:])\n",
    "            if group not in groups:\n",
    "                groups.append(group)\n",
    "            color = so_many_colors[groups.index(group)]\n",
    "            outfile.write(leaf.name + \"\\trange\\t\" + color + \"\\t\" + group + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### additional hmms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ IN RESULTS \n",
    "\n",
    "hyd_hmm_results = {}\n",
    "\n",
    "for hmm_result in glob.glob(rootdir + \"/metabolism/case_studies/*results\"):\n",
    "    hyd_hmm_results[os.path.basename(hmm_result).split(\".\")[0]] = parse_hmm(hmm_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in hyd_hmm_results.keys():   \n",
    "            \n",
    "    table = hyd_hmm_results[result]\n",
    "    table[\"position\"] = table.index\n",
    "    table[\"significant\"] = table[\"eval\"].apply(lambda x: x < 0.05)\n",
    "\n",
    "    #THEN PLOT\n",
    "    sns.set(font_scale=1.5)\n",
    "    sns.set_style(\"ticks\")\n",
    "    plt.figure(figsize=(10,3)) \n",
    "\n",
    "    sns.regplot(\"position\", \"score\", data=table, fit_reg=False, scatter_kws={'s':40}, color=\"blue\")\n",
    "    plt.xlabel(\"rank\")\n",
    "    plt.xticks()\n",
    "    ax2 = plt.twinx()\n",
    "    ax2.set_ylim(0, 0.05)\n",
    "    ax2.grid(False)\n",
    "    sns.regplot(\"position\", \"eval\", data=table, fit_reg=False, scatter_kws={'s':40}, ax=ax2, color=\"orange\")\n",
    "    plt.title(\"%s\" %(result))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in hyd_hmm_results.keys():\n",
    "    \n",
    "    with open(rootdir + \"/metabolism/case_studies/\" + result + \".names\", \"w\") as outfile:\n",
    "        \n",
    "        table = hyd_hmm_results[result]\n",
    "        for key,item in table.iterrows():\n",
    "            if item[\"eval\"] < 0.05:\n",
    "                outfile.write(item[\"gene\"]+\"\\n\")\n",
    "                \n",
    "    with open(rootdir + \"/metabolism/case_studies/\" + result + \".itol.txt\", \"w\") as itol:\n",
    "        \n",
    "        itol.write(\"DATASET_SIMPLEBAR\\nSEPARATOR COMMA\\nDATASET_LABEL,hmm_score\\nSHOW_VALUE,1\\nCOLOR,#ff0000\\nDATA\\n\")\n",
    "        table = hyd_hmm_results[result]\n",
    "        for key,item in table.iterrows():\n",
    "            if item[\"eval\"] < 0.05:\n",
    "                itol.write(item[\"gene\"] + \",\" + str(item[\"score\"]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sulf_cutoffs = {\"Fer4_22\": 34.4, \"Oxidored_q6\": 0, \"NAD_binding_1\":0}\n",
    "\n",
    "sulf_filt = {}\n",
    "\n",
    "for key in hyd_hmm_results.keys():\n",
    "    \n",
    "    table = hyd_hmm_results[key]\n",
    "    filt_table = table[(table[\"eval\"] < 0.05) & (table[\"score\"] > sulf_cutoffs[key])]\n",
    "    sulf_filt[key] = filt_table\n",
    "    \n",
    "    # write out names to match with protein fams\n",
    "    with open(rootdir + \"/metabolism/case_studies/\" + key + \".filtered.names.txt\", \"w\") as outfile:\n",
    "        \n",
    "        for keye, rowe in filt_table.iterrows():\n",
    "            outfile.write(rowe[\"gene\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyd = base_hmms[[\"name\",\"Hydrogenase_Group_3b\"]]\n",
    "hyd = hyd[hyd[\"Hydrogenase_Group_3b\"]!=\"None\"]\n",
    "hyd[\"bin\"] = hyd[\"Hydrogenase_Group_3b\"].apply(lambda x: retrieve_bin(scaffold(x), scaf2bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sulf_filt.keys():\n",
    "    \n",
    "    table = sulf_filt[key]\n",
    "    table[\"bin\"] = table[\"gene\"].apply(lambda x: retrieve_bin(scaffold(x), scaf2bin))\n",
    "    table_sub = table[[\"bin\", \"gene\"]]\n",
    "    table_sub.columns = [\"bin\",key]\n",
    "    hyd = hyd.merge(table_sub, on=\"bin\", how=\"left\").fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proximity(row):\n",
    "    \n",
    "    return len(set(scaffold(row[x]) for x in [\"Hydrogenase_Group_3b\",\"Fer4_22\",\"Oxidored_q6\", \"NAD_binding_1\"]))\n",
    "\n",
    "# get # unique scafs\n",
    "hyd[\"scaf_count\"] = hyd.apply(lambda x: get_proximity(x), axis=1)\n",
    "# sort low to hihgh\n",
    "hyd_sort = hyd.sort_values([\"scaf_count\", \"bin\"], ascending=[True,True])\n",
    "# remove duplicates w/ more scaffolds - taking best\n",
    "hyd_drep = hyd_sort.drop_duplicates(subset=[\"name\", \"Hydrogenase_Group_3b\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out shapes for hyd, asrAB\n",
    "with open(rootdir + \"/metabolism/case_studies/sulf_hmms.txt\",\"w\") as outfile:\n",
    "    outfile.write(\"DATASET_BINARY\\nSEPARATOR COMMA\\nDATASET_LABEL,sulfhydrogenase_components\\nCOLOR,#ff0000\\nFIELD_SHAPES,2,2,2\\nFIELD_LABELS,\")\n",
    "    outfile.write(\"Oxidored_q6,Fer4_22,NAD_binding_1\\n\")\n",
    "    outfile.write(\"FIELD_COLORS,#1b9e77,#d95f02,#7570b3\\nDATA\\n\")\n",
    "    \n",
    "    for key, row in hyd_drep.iterrows():\n",
    "        outfile.write(row[\"Hydrogenase_Group_3b\"] + \",\")\n",
    "        block = \"\"\n",
    "        for gene in [\"Oxidored_q6\", \"Fer4_22\", \"NAD_binding_1\"]:\n",
    "            # same scaffold\n",
    "            if scaffold(row[gene]) == scaffold(row[\"Hydrogenase_Group_3b\"]):\n",
    "                block += \"1,\"\n",
    "            # different scaffold\n",
    "            elif row[gene] != \"None\":\n",
    "                block += \"0,\"\n",
    "            # none\n",
    "            else: block += \"-1,\"\n",
    "        outfile.write(block.strip(\",\") + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(rootdir + \"/metabolism/case_studies/sulf_hmms_tax.itol.txt\",\"w\") as outfile:\n",
    "    outfile.write(\"DATASET_COLORSTRIP\\nSEPARATOR TAB\\nDATASET_LABEL\\ttaxonomy\\nDATA\\n\")\n",
    "    for key, row in base_hmms.iterrows():\n",
    "        if row[\"Hydrogenase_Group_3b\"] != \"None\":\n",
    "            htype = \"3b\"\n",
    "        elif row[\"Hydrogenase_Group_4\"] != \"None\":\n",
    "            htype = \"4\"\n",
    "        else: htype = None\n",
    "        if htype and row[\"revised_tax\"] in mapping:\n",
    "            color = tax2color[mapping[row[\"revised_tax\"]]]\n",
    "            outfile.write(row[\"Hydrogenase_Group_\" + htype]+ \"\\t\" + color + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### genomic context analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build contig db for all proteins\n",
    "contig_db = {}\n",
    "\n",
    "for record in SeqIO.parse(open(rootdir + \"/proteins/all_proteins.faa\"), \"fasta\"):\n",
    "    m = re.search(\"(\\S+) # ([0-9]+) # ([0-9]+) # ([1-]+) .+\", record.description)\n",
    "    scaf = scaffold(m.group(1))\n",
    "    if scaf not in contig_db:\n",
    "        contig_db[scaf] = [m.group(1)]\n",
    "    else:\n",
    "        contig_db[scaf].append(m.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define foci and look in neighborhood\n",
    "foci = list(base_hmms.query(\"Hydrogenase_Group_3b != 'None'\")[\"Hydrogenase_Group_3b\"])\n",
    "# define search radius\n",
    "radius = 20\n",
    "\n",
    "neighbors = {}\n",
    "for focus in foci:\n",
    "    gene_array = contig_db[scaffold(focus)]\n",
    "    # define span\n",
    "    upper_bound = min(len(gene_array)-1, gene_array.index(focus) + radius)\n",
    "    lower_bound = max(0, gene_array.index(focus) - radius)\n",
    "    # add results to dict\n",
    "    for i in range(lower_bound, upper_bound+1):\n",
    "        # don't include focus\n",
    "        if i != gene_array.index(focus):\n",
    "            neighbors[gene_array[i]] = {\"position\": i-gene_array.index(focus), \"focus\":focus}\n",
    "\n",
    "neighbor_df = pd.DataFrame.from_dict(neighbors, orient=\"index\")\n",
    "neighbor_df = neighbor_df.reset_index()\n",
    "neighbor_df.columns = [\"gene\", \"position\", \"focus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust position by strand orientation\n",
    "neighbor_df[\"gene_strand\"] = neighbor_df[\"gene\"].apply(lambda x: strand_dict[x])\n",
    "neighbor_df[\"focus_strand\"] = neighbor_df[\"focus\"].apply(lambda x: strand_dict[x])\n",
    "neighbor_df[\"adj_position\"] = neighbor_df.apply(lambda x: int(x[\"position\"])*int(x[\"gene_strand\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust for few cases where hydrogenase is split\n",
    "table = metabolic_hmm_results_filtered[\"Hydrogenase_Group_3b\"]\n",
    "table[\"scaf\"] = table[\"gene\"].apply(lambda x: scaffold(x))\n",
    "counts = table[\"scaf\"].value_counts().reset_index()\n",
    "dups = counts[counts[\"scaf\"]>1][\"index\"].to_list()\n",
    "\n",
    "def correct_dups(row):\n",
    "    \n",
    "    if scaffold(row[\"focus\"]) in dups:\n",
    "        \n",
    "        # get gene id of second piece\n",
    "        pieces = table[table[\"gene\"].str.contains(scaffold(row[\"focus\"]))][\"gene\"].to_list()\n",
    "        dropped = pieces[pieces != row[\"focus\"]]\n",
    "        # get relationship between dropped and focus\n",
    "        rel = neighbor_df[neighbor_df[\"gene\"]==dropped].iloc[0][\"adj_position\"]\n",
    "        \n",
    "        # modify upstream/downstream genes accordingly\n",
    "        if rel > 0 and row[\"adj_position\"] > 0:\n",
    "            return row[\"adj_position\"] - 1\n",
    "        elif rel < 0 and row[\"adj_position\"] < 0:\n",
    "            return row[\"adj_position\"] - 1\n",
    "        else: return row[\"adj_position\"]\n",
    "    \n",
    "    else:\n",
    "        return row[\"adj_position\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_df[\"cor_position\"] = neighbor_df.apply(correct_dups, axis=1)\n",
    "# get rid of extra pieces from table\n",
    "neighbor_df = neighbor_df[neighbor_df[\"cor_position\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out genes for protein clustering\n",
    "cmdir(rootdir + \"/proteins/protein_clustering/\")\n",
    "\n",
    "with open(rootdir + \"/proteins/protein_clustering/neighbors.txt\", \"w\") as outfile:\n",
    "    \n",
    "    for key, row in neighbor_df.iterrows():\n",
    "        outfile.write(row[\"gene\"] + \"\\n\")\n",
    "\n",
    "call = pspath + \" -n \" + rootdir + \"/proteins/protein_clustering/neighbors.txt -i \" + \\\n",
    "        rootdir + \"/proteins/all_proteins.faa > \" + \\\n",
    "        rootdir + \"/proteins/protein_clustering/neighbors.faa\"\n",
    "sp.call(call, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by subfamily clustering\n",
    "call = pcpath + \"/subfamilies.py --output-directory \" + \\\n",
    "    rootdir + \"/proteins/protein_clustering/output/ --cpu 6 \" + \\\n",
    "    rootdir + \"/proteins/protein_clustering/neighbors.faa\"\n",
    "sp.call(call, shell=True)\n",
    "#print(call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then do hmm-hmm comparison to generate families\n",
    "call1 = pcpath + \"/hhblits.py --cpu 6 \" + rootdir + \"/proteins/protein_clustering/output/config.json\"\n",
    "print(call1)\n",
    "#sp.call(call1, shell=True)\n",
    "call2 = pcpath + \"/runningMclClustering.py --coverage 0.50 --fasta --cpu 6 \" + rootdir + \"/proteins/protein_clustering/output/config.json\"\n",
    "#sp.call(call2, shell=True)\n",
    "print(call2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate with family stuff (see miscellaneous)\n",
    "fams = {}\n",
    "\n",
    "count = 1\n",
    "for line in open(rootdir + \"/proteins/protein_clustering/output/orf2family.tsv\").readlines():\n",
    "    # skip headers\n",
    "    if count != 1:\n",
    "        splt = line.strip().split(\"\\t\")\n",
    "        fams[splt[0]] = splt[1]\n",
    "    count +=1\n",
    "\n",
    "# map subfams to neighbors\n",
    "neighbor_df[\"fam\"] = neighbor_df[\"gene\"].apply(lambda x: fams[x] if x in fams else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in phylogenetic type\n",
    "types = {}\n",
    "\n",
    "for file in glob.glob(rootdir + \"/metabolism/case_studies/hyd_*.txt\"):\n",
    "    for line in open(file).readlines():\n",
    "        types[line.strip()] = os.path.basename(file).split(\".\")[0]\n",
    "\n",
    "neighbor_df[\"type\"] = neighbor_df[\"focus\"].apply(lambda x: types[x] if x in types else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_colors = {\"Oxidored_q6\": \"#1b9e77\",\"Fer4_22\":\"#d95f02\",\"NAD_binding_1\":\"#7570b3\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlate subfams + hmms\n",
    "family2hmm = {}\n",
    "\n",
    "top_families = list(neighbor_df[\"fam\"].value_counts()[0:15].index)\n",
    "top_families = [top for top in top_families if top !=\"None\"]\n",
    "\n",
    "for family in top_families:\n",
    "    \n",
    "    genes = neighbor_df[neighbor_df[\"fam\"]==family][\"gene\"].to_list()\n",
    "\n",
    "    for hmm in sulf_filt.keys():\n",
    "        db = sulf_filt[hmm][\"gene\"].to_list()\n",
    "        overlap = set(genes).intersection(set(db))\n",
    "        if len(overlap)/float(len(genes)) > 0.5:\n",
    "            family2hmm[family] = hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define subset and groupby\n",
    "for phy in neighbor_df[\"type\"].unique():\n",
    "    \n",
    "    if phy != \"None\":\n",
    "        phy_df = neighbor_df[(neighbor_df[\"type\"]==phy)  & (neighbor_df[\"gene_strand\"]==neighbor_df[\"focus_strand\"])]\n",
    "        top_families = list(phy_df[\"fam\"].value_counts()[0:7].index)\n",
    "        top_families = [top for top in top_families if top !=\"None\"]\n",
    "        \n",
    "        # make color palette\n",
    "        family2color = {}\n",
    "        for family in top_families:\n",
    "            if family in family2hmm:\n",
    "                family2color[family] = fam_colors[family2hmm[family]]\n",
    "            else:family2color[family] = \"#d3d3d3\"\n",
    "        ng = phy_df[(phy_df[\"fam\"].isin(top_families))].groupby([\"fam\", \"cor_position\"], as_index=False).count()\n",
    "        ng = ng[[\"fam\", \"cor_position\", \"focus\"]]\n",
    "        ng.columns = [\"fam\", \"cor_position\", \"count\"]\n",
    "        # restrict radius\n",
    "        ng = ng[abs(ng[\"cor_position\"])<=10]\n",
    "        \n",
    "        sns.set(font_scale=1)\n",
    "        sns.set_style(\"white\", {\"axes.edgecolor\": \"0.8\"})\n",
    "        kws = dict(linewidth=.5, edgecolor=\"black\")\n",
    "        g = sns.relplot(\"cor_position\", \"fam\", data=ng, size=\"count\", hue=\"fam\",palette=family2color, alpha=1, height=2.5, aspect=3, sizes=(50,500), **kws, legend=\"brief\")\n",
    "        #for i in range(min(ng[\"cor_position\"])-1,max(ng[\"cor_position\"]+1)):\n",
    "        for i in range(-10,11,1):\n",
    "            if i >= 0 and i < 6:\n",
    "                plt.axhline(i, color='grey', linestyle='-', lw=0.5,zorder=0)\n",
    "            if i==0:\n",
    "                plt.axvline(i, color='black', linestyle='-', lw=2,zorder=1)\n",
    "            else:\n",
    "                plt.axvline(i, color='grey', linestyle='-', lw=0.5,zorder=0)\n",
    "        \n",
    "        def round(x):\n",
    "            return math.ceil(x / 2.) * 2\n",
    "        \n",
    "        #g.set(xticks=[i for i in range(round(min(ng[\"cor_position\"])-1),round(max(ng[\"cor_position\"])+1),2)])\n",
    "        g.set(xticks=[i for i in range(-10,11,2)])\n",
    "        plt.ylabel(\"\")\n",
    "        plt.xlabel(\"relative gene position\")\n",
    "        sns.despine(left=False, bottom=True, top=True, right=False)\n",
    "        plt.savefig(rootdir + \"/figures/\" + phy + \"_context.svg\", format=\"svg\")\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyd small subunit tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create alignment\n",
    "call = mpath + \" --thread 8 --reorder \" + rootdir + \"/proteins/protein_clustering/output/familiesFasta/fam019.fa > \" + \\\n",
    "    rootdir + \"/metabolism/case_studies/hyd_SSU.mafft\"\n",
    "print(call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. Uses qsub, modify below command if necessary. Use Geneious to strip gene alignment as described above and create new *stripped* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and run the trees\n",
    "for aln in glob.glob(rootdir + \"/metabolism/case_studies/*SSU*stripped.mafft\"):\n",
    "    \n",
    "    basename = os.path.basename(aln).split(\".\")[0]\n",
    "    call = \"echo '\" + iqpath + \" -s \" + aln + \" -m TEST -st AA -bb 1500 -nt 48 -pre \" + aln.split(\".\")[0] + \"' | qsub -V -N \" + basename\n",
    "    #sp.call(call, shell=True)\n",
    "    print(call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create itol \n",
    "with open(rootdir + \"/metabolism/case_studies/hyd_SSU.itol.txt\", \"w\") as outfile:\n",
    "    \n",
    "    outfile.write(\"DATASET_ALIGNMENT\\nSEPARATOR COMMA\\nDATASET_LABEL,hyd_SSU_aln\\nCOLOR,#ff0000\\nCUSTOM_COLOR_SCHEME,MY_SCHEME_1,A=#d2d0c9,M=#d2d0c9,I=#d2d0c9,L=#d2d0c9,V=#d2d0c9,P=#746f69,G=#746f69,C=#746f69,F=#d0ad16,Y=#d0ad16,W=#d0ad16,S=#34acfb,T=#34acfb,N=#34acfb,-=#ffffff,Q=#34acfb,R=#34fb54,K=#34fb54,H=#34fb54,D=#fb4034,E=#fb4034\\nDATA\\n\")\n",
    "    \n",
    "    for record in SeqIO.parse(open(rootdir + \"/metabolism/case_studies/hyd_SSU.stripped.mafft\"), \"fasta\"):\n",
    "        outfile.write(\">\" + record.description.split(\" \")[0] + \"\\n\" + str(record.seq) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write itol ranges\n",
    "with open(rootdir + \"/metabolism/case_studies/hyd_SSU.ranges.txt\", \"w\") as out:\n",
    "    \n",
    "    out.write(\"TREE_COLORS\\nSEPARATOR TAB\\nDATA\\n\")\n",
    "    for key, row in neighbor_df.iterrows():\n",
    "        \n",
    "        if row[\"fam\"] == \"fam019\":\n",
    "            if row[\"type\"] == \"hyd_1\":\n",
    "                out.write(row[\"gene\"] + \"\\trange\\tlightgrey\\thyd_SSU\\n\")\n",
    "            else: out.write(row[\"gene\"] + \"\\trange\\twhite\\thyd_SSU\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 - ajaffe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
